{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-zNP2RMJW8X"
   },
   "source": [
    "# Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76851,
     "status": "ok",
     "timestamp": 1623853975566,
     "user": {
      "displayName": "Ryan Cohn",
      "photoUrl": "",
      "userId": "01706064865196246875"
     },
     "user_tz": 240
    },
    "id": "Z4RXXMzIJcz8",
    "outputId": "8bcca4df-e357-4265-ad8b-4bf9956a9d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling torch-1.6.0+cu101:\n",
      "  Successfully uninstalled torch-1.6.0+cu101\n",
      "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.6.0+cu101\n",
      "  Using cached https://download.pytorch.org/whl/cu101/torch-1.6.0%2Bcu101-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (1.19.5)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.6.0+cu101\n",
      "Looking in links: https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.6.0.html\n",
      "Collecting torch-scatter==latest+cu101\n",
      "  Using cached https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.6.0/torch_scatter-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl\n",
      "Collecting torch-sparse==latest+cu101\n",
      "  Using cached https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.6.0/torch_sparse-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse==latest+cu101) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse==latest+cu101) (1.19.5)\n",
      "Installing collected packages: torch-scatter, torch-sparse\n",
      "  Found existing installation: torch-scatter 2.0.5\n",
      "    Uninstalling torch-scatter-2.0.5:\n",
      "      Successfully uninstalled torch-scatter-2.0.5\n",
      "  Found existing installation: torch-sparse 0.6.8\n",
      "    Uninstalling torch-sparse-0.6.8:\n",
      "      Successfully uninstalled torch-sparse-0.6.8\n",
      "Successfully installed torch-scatter-2.0.5 torch-sparse-0.6.8\n",
      "Looking in links: https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.6.0.html\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch-spline-con==latest+cu101 (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torch-spline-con==latest+cu101\u001b[0m\n",
      "Looking in links: https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.6.0.html\n",
      "Collecting torch-sparse==latest+cu101\n",
      "  Using cached https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.6.0/torch_sparse-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse==latest+cu101) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse==latest+cu101) (1.19.5)\n",
      "Installing collected packages: torch-sparse\n",
      "  Found existing installation: torch-sparse 0.6.8\n",
      "    Uninstalling torch-sparse-0.6.8:\n",
      "      Successfully uninstalled torch-sparse-0.6.8\n",
      "Successfully installed torch-sparse-0.6.8\n",
      "Requirement already satisfied: torch-geometric==1.6.1 in /usr/local/lib/python3.7/dist-packages (1.6.1)\n",
      "Requirement already satisfied: ase in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (3.21.1)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (0.51.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (1.6.0+cu101)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (2.23.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (1.1.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (2.11.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (1.19.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (4.41.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (0.22.2.post1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (2.5.1)\n",
      "Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (5.0.0)\n",
      "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (0.4)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.6.1) (3.1.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ase->torch-geometric==1.6.1) (3.2.2)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->torch-geometric==1.6.1) (0.34.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->torch-geometric==1.6.1) (57.0.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric==1.6.1) (0.16.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==1.6.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==1.6.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==1.6.1) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==1.6.1) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric==1.6.1) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric==1.6.1) (2018.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric==1.6.1) (2.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric==1.6.1) (1.0.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->torch-geometric==1.6.1) (4.4.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric==1.6.1) (1.15.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric==1.6.1) (2.4.7)\n",
      "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric==1.6.1) (0.6.0)\n",
      "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->torch-geometric==1.6.1) (1.5.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric==1.6.1) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric==1.6.1) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Please visit https://github.com/rusty1s/pytorch_geometric#pip-wheels for lastest installation instruction\n",
    "\n",
    "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html -U\n",
    "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html -U\n",
    "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html -U\n",
    "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html -U\n",
    "!pip install torch-geometric -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9htnPB9fZ7xA"
   },
   "source": [
    "# Loading Datasets\n",
    "For our datasets, we will be using three citation networks; Pubmed, Cora and Citeseer. Nodes correspond to publications and edges correspond to citations. The citation networks are available through the Planetoid dataset of PyG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8197,
     "status": "ok",
     "timestamp": 1623853983752,
     "user": {
      "displayName": "Ryan Cohn",
      "photoUrl": "",
      "userId": "01706064865196246875"
     },
     "user_tz": 240
    },
    "id": "oArjAIk5Jdhd",
    "outputId": "121ca92f-a159-46cb-f1c5-32c796baf8aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation network information\n",
      "Cora:  Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])\n",
      "Citeseer:  Data(edge_index=[2, 9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])\n",
      "Pubmed:  Data(edge_index=[2, 88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "#Load the Cora, CiteSeer and Pubmed citation networks\n",
    "#Note: T.NormalizeFeatures() creates a transform that normalizes the node features\n",
    "dataset_cora = Planetoid(root=\"./tmp\", name=\"Cora\", transform=T.NormalizeFeatures())\n",
    "dataset_citeseer = Planetoid(root=\"./tmp\", name=\"CiteSeer\", transform=T.NormalizeFeatures())\n",
    "dataset_pubmed = Planetoid(root=\"./tmp\", name=\"Pubmed\",transform=T.NormalizeFeatures())\n",
    "\n",
    "data_cora = dataset_cora[0]\n",
    "data_citeseer = dataset_citeseer[0]\n",
    "data_pubmed = dataset_pubmed[0]\n",
    "\n",
    "print(\"Citation network information\")\n",
    "print(\"Cora: \", data_cora)\n",
    "print(\"Citeseer: \", data_citeseer)\n",
    "print(\"Pubmed: \", data_pubmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_OFWvzze2ri"
   },
   "source": [
    "# Training & Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1623853983753,
     "user": {
      "displayName": "Ryan Cohn",
      "photoUrl": "",
      "userId": "01706064865196246875"
     },
     "user_tz": 240
    },
    "id": "IWVLX-zPejyn"
   },
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "  # Set the model.training attribute to True\n",
    "  model.train() \n",
    "\n",
    "  # Reset the gradients of all the variables in a model\n",
    "  optimizer.zero_grad() \n",
    "\n",
    "  # Get the output of the network. The output is a log probability of each\n",
    "  log_softmax = model(data) \n",
    "\n",
    "  labels = data.y # Labels of each node\n",
    "\n",
    "  # Use only the nodes specified by the train_mask to compute the loss.\n",
    "  nll_loss = F.nll_loss(log_softmax[data.train_mask], labels[data.train_mask])\n",
    "  \n",
    "  #Computes the gradients of all model parameters used to compute the nll_loss\n",
    "  #Note: These can be listed by looking at model.parameters()\n",
    "  nll_loss.backward()\n",
    "\n",
    "  # Finally, the optimizer looks at the gradients of the parameters \n",
    "  # and updates the parameters with the goal of minimizing the loss.\n",
    "  optimizer.step() \n",
    "\n",
    "def compute_accuracy(model, data, mask):\n",
    "  # Set the model.training attribute to False\n",
    "  model.eval()\n",
    "  logprob = model(data)\n",
    "  _, y_pred = logprob[mask].max(dim=1)\n",
    "  y_true=data.y[mask]\n",
    "  acc = y_pred.eq(y_true).sum()/ mask.sum().float()\n",
    "  return acc.item()\n",
    "\n",
    "@torch.no_grad() # Decorator to deactivate autograd functionality  \n",
    "def test(model, data):\n",
    "  acc_train = compute_accuracy(model, data, data.train_mask)\n",
    "  acc_val = compute_accuracy(model, data, data.val_mask)\n",
    "\n",
    "  return acc_train, acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eams8ptHOf3S"
   },
   "source": [
    "# Graph Attention Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iB-rQ1aOOh6G"
   },
   "source": [
    "In this notebook we will be using the graph attention (GAT) convolutional operator.   On application of this operator on the graph, each node's feature-vector at layer $k$ is computed by \n",
    "\n",
    "$$ \\mathbf{v}_i^{(k)} = \\sigma \\left( \\sum_{v_j\\in N(v_i)} \\alpha_{ij} W \\mathbf{v}_j^{(k-1)}\\right)$$. \n",
    "\n",
    "Let's break this equation down.  \n",
    "\n",
    "1. First each of neighboring-node feature-vectors  are multiplied by a weight matrix $W$ resulting in   the term $W \\mathbf{v}_j^{(k-1)}$. We can imagine this as a message that is sent to node $v_i$ from each of its neighboring nodes. \n",
    "\n",
    "2. Next, we take a weighted average of these messages, where the weights are given by a function of the node feature-vectors and the weight matrix $W$, i.e. $a_{ij}=f(v_i, v_j, W)$. This results in the term $\\sum_{v_j\\in N(v_i)} \\alpha_{ij} W \\mathbf{v}_j^{(k-1)}$. \n",
    "\n",
    "3. Finally we apply a nonlinear function to this weighted average..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdH5JuoPTzlq"
   },
   "source": [
    "## Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdM-Bm6zT169"
   },
   "source": [
    "\n",
    "Let's take a closer look at the weights $a_{ij}$ are computed.\n",
    "\n",
    "First we compute some unnormalized weight that depends on the weight matrix $W$, and the feature-vectors:\n",
    "\n",
    "$$\\rho_{ij} = \\sigma\\left( \n",
    "  \\mathbf{a}^T\\left[ \n",
    "    W\\mathbf{v}_i^{(k-1)}||W\\mathbf{v}_j^{(k-1)}\\right]\n",
    "  \\right).$$\n",
    "Here $\\mathbf{a}$ is a weight vector that needs to be learned, and $||$ is a vector concatenation operation.\n",
    "\n",
    "\n",
    "<center><img src=\"https://ai.science/api/authorized-images/P9YAB%2FiojuBjYHys2SyLct2iT%2B6CYnF565SUHuvCwGbpd60fhuqfp6b%2BPq5FgfGFF%2BxGt5dvNdCaHWAdd%2F%2BxCfvnJDQ6VZgzYCElgNjc5wFCXII7dM2jFI8xKMTG2oEuUCwUdRtubT%2BELMsTjzLHtZ0aJCNyJKWxCaAzj37IqamCwdNtgzJ8rpnC69%2BUj3L%2FnovLaA3OWOgMFnSGWSB35Pi0L4ZzqI7gGpVScFZl2OZB1MEnR9oJkIqP4oYgXY3%2BwRdP8lrG0nhNTBzfEbC6gpiU7WQTdqKAbyAzBsLgA5Kv%2FR%2BZhpccdktppsCdSoAK0yBvjaulNzUuHVNvLPBLaakBCOScsvqUSk9RruKYugkMDG%2BTgowM3Qmm772Zw%2FpqOxUdJUaohlb1Hz0nYK1FUJHu5ubj6fKW5JC8e024rg%2F2mtrQk1GgYISH7tpomBbNxRPX5QKJtJ54cSME3JSfcEvDBhaIBprK8FvdZwti1BiizeBXphLw1FaaeYeQBnmN2dWTSs28QhOSSs6jArkD%2Fa9ixN%2F6iI9zO59IBGA8undgK%2BayjCsYjWwuCI81lCWnvEIVFDCXnden6%2BWnIjPrH1uYoGxjmjMPsUoW%2BJio%2B%2FocC8C%2FNf4FUfPS8S29nE28rm1cKZDtQxacLEB0kJorUKe4jHRFsmOWHl2zXM9DvkY%3D\" width=\"20%\" > </center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Next we ensure that the weights for each node $v_i$ sum up to equal to 1 by applying a softmax function\n",
    "\n",
    "$$ \\alpha_{ij} = \\frac{exp(\\rho_{ij})}\n",
    "{ \\sum_{j \\in N(v_i)} exp(\\rho_{ij})}.$$\n",
    "\n",
    "\n",
    "What we described so far is one attention head. We can also add multiple attention heads, each giving a different weight $a_{ij}$, resulting in different weighted averages being used in the GAT convolution operator. This is analogous to learning multiple convolutional filters in CNNS.  We can aggregate the outputs of resulting operations using different attention heads by either taking the average  on concatenating them.\n",
    "\n",
    "<center><img src=\"https://ai.science/api/authorized-images/xU0Yv9p%2BCBULhnI9YGFPXt1QXea%2F492Zz5mauFoOKPCu7GcU%2B0VTbvvXP4hmoWW1K4AppfcZFfpkgcbvi%2FFt1ibHtHfCOL8r7PvtsG2Gr3cvKcL34Duli1lrrVM%2BW%2FJ0b1Dg%2Fw8tFynbEEzKYNE5spRONapqFRw2DOeyYFTF8ifxLDg6kQ3Jw1B5naTrSE%2FB%2Bp4p4Nsc1jf6Ij9nHZxr%2Fzx4%2Br5hbK9OGDNPrvatD61EhxdPiFiIV8ttMPRVUpSDXJMnCqS21cqS4Ws502VpD%2F2AxxI%2FXurz38O2uHLCpCLp%2BcrhP6n7%2B6PZCF7H%2BeLySasjT3C0%2F4ERDnR7z5cHcjeK54AeFhZ9bIZtymxyclg%2BLtniQBdPUhKeoC7W%2FY3Tlsa0Y%2Bn4Z5WRf9fm5zLWGH1kVIL8AawRPUwdfn8ZdBvCzHw6PCjFcp4BT2WbXlae9DLhNjzBs7bdbrvPhmuuJ7mvQd5QLHsRWytTO%2BTr9k9hOlGos0a48MKjl8Gzn5ll3cBtwEFHGmq9WP7TUiidBxtLnLepxFO82vBlIKikwo9yD4y5ZeJiQfX9UUNa1pNZcGgHcgHFeNSYCJW0EqbZeoxL13znoJT7DZwbryjiTKdlRaO9c%2B%2Fmjozyz0rIXNqEslvyJzXFnTPwZN0dGVt5BlNBK7ruC4n%2Bn9P3GA2tZE8%3D\" width=\"40%\" > </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8cGwlpap5Im"
   },
   "source": [
    "# Graph Attention Networks in PyG\n",
    "We will now implement a network based on the graph attention convolutional layer described in the paper [Graph Attention Networks](https://arxiv.org/abs/1710.10903) and provided in PyG as the *GATConv* layer. Note that this layer takes different parameters than *SGConv*. All layers have their own unique set of parameters, which are described in [PyG's documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1623853983754,
     "user": {
      "displayName": "Ryan Cohn",
      "photoUrl": "",
      "userId": "01706064865196246875"
     },
     "user_tz": 240
    },
    "id": "CZbzMQ7N-75o"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GATNet(torch.nn.Module):\n",
    "  def __init__(self, data, heads_layer1, \n",
    "               heads_layer2, dropout, dropout_alphas):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dropout=dropout\n",
    "    num_features = data.num_features\n",
    "    num_classes = len(data.y.unique())\n",
    "\n",
    "    self.conv1 = GATConv(in_channels=num_features, out_channels=8,\n",
    "                         heads=heads_layer1, concat=True, negative_slope=0.2, \n",
    "                         dropout=dropout_alphas)\n",
    "    \n",
    "    self.conv2 = GATConv(in_channels=8*heads_layer1, out_channels=num_classes, \n",
    "                         heads=heads_layer2, concat=False, negative_slope=0.2,\n",
    "                         dropout=dropout_alphas)\n",
    "  \n",
    "  def forward(self, data):\n",
    "      x=data.x\n",
    "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "      x = self.conv1(x, data.edge_index)\n",
    "      x = F.elu(x)\n",
    "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "      x = self.conv2(x, data.edge_index)\n",
    "      \n",
    "      return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usoP8KBEzyx1"
   },
   "source": [
    "We can use the previously written training and testing code to try out this model to classify nodes in the PubMed dataset. We use the default parameters from the paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 161975,
     "status": "ok",
     "timestamp": 1623854145702,
     "user": {
      "displayName": "Ryan Cohn",
      "photoUrl": "",
      "userId": "01706064865196246875"
     },
     "user_tz": 240
    },
    "id": "6hpzboYUdTVt",
    "outputId": "b03a1918-d07b-4cb0-c490-70d07aa9939e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train: 0.8333, Val: 0.6320\n",
      "Epoch: 020, Train: 0.8833, Val: 0.7040\n",
      "Epoch: 030, Train: 0.8833, Val: 0.7300\n",
      "Epoch: 040, Train: 0.9000, Val: 0.7360\n",
      "Epoch: 050, Train: 0.9000, Val: 0.7360\n",
      "Epoch: 060, Train: 0.9000, Val: 0.7500\n",
      "Epoch: 070, Train: 0.9000, Val: 0.7400\n",
      "Epoch: 080, Train: 0.9000, Val: 0.7380\n",
      "Epoch: 090, Train: 0.8833, Val: 0.7400\n",
      "Epoch: 100, Train: 0.9000, Val: 0.7400\n",
      "Epoch: 110, Train: 0.9000, Val: 0.7500\n",
      "Epoch: 120, Train: 0.9000, Val: 0.7400\n",
      "Epoch: 130, Train: 0.9000, Val: 0.7440\n",
      "Epoch: 140, Train: 0.9000, Val: 0.7520\n",
      "Epoch: 150, Train: 0.9167, Val: 0.7480\n",
      "Epoch: 160, Train: 0.9167, Val: 0.7520\n",
      "Epoch: 170, Train: 0.9167, Val: 0.7520\n",
      "Epoch: 180, Train: 0.9167, Val: 0.7520\n",
      "Epoch: 190, Train: 0.9167, Val: 0.7520\n",
      "Epoch: 200, Train: 0.9167, Val: 0.7500\n"
     ]
    }
   ],
   "source": [
    "# Set cuda to be the device if available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_pubmed_gat = GATNet(data=data_pubmed, heads_layer1=8, heads_layer2=8, \n",
    "                          dropout=0.6,  dropout_alphas=0.6).to(device)\n",
    "data_pubmed= data_pubmed.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_pubmed_gat.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(1, 200+1):\n",
    "    train(model_pubmed_gat, data_pubmed, optimizer)\n",
    "    if epoch %10 ==0:\n",
    "      log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}'\n",
    "      print(log.format(epoch, *test(model_pubmed_gat,data_pubmed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9auZ-qeztz_"
   },
   "source": [
    "# Optional Exercise 1\n",
    "\n",
    "* Study the effect of changing the different hyper parameters of the GAT Network above.\n",
    "* Try to reproduce (or do better than) the results in the GAT paper, which are reproduced below. Hint: you may need to also change the parameters on the optimizer and not just the model.\n",
    "\n",
    "<center><img src=\"https://ai.science/api/authorized-images/m40NptgQCvGge0XNb3KZWoOcsvBpo0r3qQ6yOIYJngqNByMQ%2FS8CGoURLG3uP61rQ6fgybshIDDm1Wnyl1ni0nC6vfZvaAaL9CJF2zm4Jr7LAZ6TCBYMkyk7xFqkMeIHFYvigZomO1CgTf2%2F8OOlyx5gXe7HV9Be1Qm5iaGIbWepuJ26j1VZC7x8HtQUUlVG0yCD95d70ZP6839I%2FZb7oIve42o%2B3BrYnvPbR01ocLqaRfRWSFhmPJArU92uPlDnrKmk0qS3NBneD9zZyqs2wYJEKub7mxbJdJtsy%2BJAZEHUMM9%2B8SlKnWtTc4e%2ByAIlO6ihENeTNbMygcPOT%2Bclk2JUkjLoBtVVawHgB1Q0DiauUUsRIlIcQsQALOo1hawiRToCd%2FpdRr5PnL1XeKml%2BusObnO%2BJ3e6MuuiMSEyFMyNY5DeYMSJt5xSQ7aX%2Bl3943PRW72stYXCDw%2F6RCW6voz4VON%2BMqQes64R8yvjBsUcfw8irsoH0kNei6CB0LoZMxY2fPuhe%2FnR80n4gJwyUOcMnCvqmSAdnpKR9fok4UDgnyd9nOw5eny3hMkb%2Ff%2Bt2ffqsUgaPse24Q9rAmKOR0JUAOQ9Orygbql%2FKWoF%2FGnR%2FJoR0NPrru8vFTz8eqhqw9J%2FlO9HXAuCre5NecGr48O514kg6Dt070GkX3chrL8%3D\" width=\"80%\" > </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyvX4QbY20sJ"
   },
   "source": [
    "# Optional Exercise 2\n",
    "* There are [many graph convolutional operators availabe in PyG.](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html)  Try to implement a network for node classification which uses the *SAGEConv* convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ju2FZaF9cTe0"
   },
   "source": [
    "# References\n",
    "[Graph Attention Networks Site](https://petar-v.com/GAT/)\n",
    "\n",
    "\n",
    " [Veličković, Petar, et al. \"Graph attention networks.\" arXiv preprint arXiv:1710.10903 (2017)](https://arxiv.org/abs/1710.10903)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_-zNP2RMJW8X"
   ],
   "name": "2_node_classification_GAT_check_final.ipynb",
   "provenance": [
    {
     "file_id": "1_HZ4DotIxXQXBi5S_lxtGj4PE32DS6oe",
     "timestamp": 1623014722643
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
