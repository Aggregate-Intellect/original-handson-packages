{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on Challenge \n",
    "\n",
    "We are going to continue with our code from last week, except there will be a few changes:\n",
    "\n",
    "-   We are exchanging the output vector of a layer with an input vector instead. This will make the code a little cleaner\n",
    "-   Layers will now be subclasses of the *layer* super class\n",
    "-   The name will now be changed to *FFNN* for *FeedForward Neural Network*, since we are expanding on what our network can do.\n",
    "\n",
    "This way our FFNN package will be like a \"mini pytorch\", and our training code will be similar to how pytorch is used. Here is the complete code for our neural network class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN: #Previous MLP class\n",
    "    def __init__(self):\n",
    "        self.net = []\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.net:\n",
    "            x = layer.forward(x)\n",
    "        self.output = x\n",
    "        return x\n",
    "\n",
    "    def backward(self, error):\n",
    "        #New\n",
    "        for layer in reversed(self.net):\n",
    "            error = layer.backward(error)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for layer in self.net:\n",
    "            layer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all there is to our network model. Nice and simple. All the rest of the work is done by the layers. We create an instance of a *FFNN* as before by appending layers to *self.net* and calling \n",
    "`model = FFNN()`\n",
    "\n",
    "Here is the *layer* superclassThat is all there is to our network model. Nice and simple. All the rest of the work is done by the layers. We create an instance of a *FFNN* as before by appending layers to *self.net* and calling \n",
    "`model = FFNN()`\n",
    "\n",
    "Here is the *layer* superclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self, node_dim):\n",
    "        \"\"\"\n",
    "        This init should be called via super() with the number\n",
    "        of nodes as an argument.\n",
    "        \"\"\"\n",
    "        self.input = np.zeros(node_dim)\n",
    "        self.input_grad = np.zeros(node_dim)\n",
    "        self.params = False #New\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return x\n",
    "\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "    def parameters(self):\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.input_grad.fill(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any layer you are responsible for:\n",
    "\n",
    "1.  initializing any additional parameters in `__init__()`. If you do, set the `self.params` flag to `True`.\n",
    "2.  implementing the `forward` method.\n",
    "3.  implementing the `backward` and, if applicable, `parameters` methods.\n",
    "\n",
    "**Step #3** can be ignored for this week, it will be needed next week when we will implement gradient descent. \n",
    "\n",
    "This is what your layers should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mylayer(layer): #previous linearlayer\n",
    "    def __init__(self, my_args, node_dim): #args added\n",
    "        super( ylayer , self).__init__(node_dim)\n",
    "        self.out = np.zeros(node_dim)\n",
    "        self.weights = np.random.rand(node_dim, in_dim)\n",
    "        self.params = True\n",
    "        # if there are parameters, instantiate here and set \n",
    "        # self.params = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.out = np.dot(self.weights, x)\n",
    "        if self.bias.any():\n",
    "            self.out += self.bias\n",
    "            \n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge for the Week \n",
    "\n",
    "Implement a 2D convolution layer. There is more than one way to calculate convolutions, but we are only asking for the most straight-forward way involving nested `for` loops. \n",
    "(Aside: Computing convolutions this way is slow, and that is not how it is implemented by current frameworks. However creating a fast implementation is beyond the scope of this workshop.) \n",
    "\n",
    "Here is a code snippet to get you started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2d(layer):\n",
    "    def __init__(self, in_c, out_c, kernel_size, stride=1, padding=0, bias=True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        in_c: Input channel. If there is an rgb image this would be 3.\n",
    "        out_c: Output channel. The desired channels in the output of the conv layer.\n",
    "        kernel_size: symmetrically kernel size i.e. 3x3.\n",
    "        stride: the kernel moves in increment of n strides.\n",
    "        padding: number of padding creating around the image.\n",
    "        \"\"\"\n",
    "        super(conv2d, self).__init__(out_c)\n",
    "        self.params = True\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.w = torch.rand([kernel_size,kernel_size,10]) # random kernels (aka weights) tensor \n",
    "\n",
    "        if bias:\n",
    "            self.b = np.zeros(in_c) # bias tensor \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        \n",
    "        \n",
    "        # calculate traversal using size of: x, kernel, stride, padding\n",
    "\n",
    "        for : # traverse 2D input vertically\n",
    "            for : # traverse input horizontally\n",
    "                # compute dot product(s)\n",
    "\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
