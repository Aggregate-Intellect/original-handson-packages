{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJi4qiGZg7IG"
   },
   "source": [
    "## Hands-on Challenge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktu7A7EQg7II"
   },
   "source": [
    "We are going to implement gradient descent and train a model. Let's get started. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xo0WGLKYg7If"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr3DVEOfg7In"
   },
   "source": [
    "You will need all the code that we have written so far. Here are the base classes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oreH2PFBg7Iq"
   },
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self):\n",
    "        self.net = []\n",
    "        self.output = None\n",
    "    def forward(self, x):\n",
    "        for layer in self.net:\n",
    "            x = layer.forward(x)\n",
    "        self.output = x\n",
    "        return x\n",
    "    def backward(self, error):\n",
    "        for layer in reversed(self.net):\n",
    "            error = layer.backward(error)\n",
    "    def zero_grad(self):\n",
    "        for layer in self.net:\n",
    "            layer.zero_grad()\n",
    "\n",
    "class layer:\n",
    "    def __init__(self, node_dim):\n",
    "        self.input = np.zeros(node_dim)\n",
    "        self.input_grad = np.zeros(node_dim)\n",
    "        self.params = False\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "    def backward(self):\n",
    "        pass\n",
    "    def parameters(self):\n",
    "        pass\n",
    "    def zero_grad(self):\n",
    "        self.input_grad.fill(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LBq5cXsg7Iw"
   },
   "source": [
    "### Exercise: implement the backward pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Iyu5M8pg7I4"
   },
   "source": [
    "Remember from last week, your layers will need to conform to the template below. You will need to implement the `backward` and `parameters` methods. \n",
    "\n",
    "**EXERCISE:** Do this for the `linear` and `sigmoid` layers. \n",
    "OPTIONAL: Do the same for any other layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3MkbKJmg7I6"
   },
   "outputs": [],
   "source": [
    "class my_layer(layer):\n",
    "    def __init__(self, node_dim):\n",
    "        super(mylayer, self).__init__(node_dim)\n",
    "        # if there are parameters: \n",
    "        # self.params = True\n",
    "\n",
    "        # TODO instantiate parameters and also include gradient tensors of the same shape, ie.\n",
    "        # self.parameter = np.array((some shape))\n",
    "        # self.parameter_grad = np.zeros((same shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return # your results\n",
    "\n",
    "    def backward(self, error):\n",
    "        self.input_grad = # TODO compute the gradient of 'x', the layers input \n",
    "\n",
    "        # TODO compute any other gradients \n",
    "\n",
    "        # back propagate loss to previous layer \n",
    "        return self.input_grad\n",
    "\n",
    "    def parameters(self):\n",
    "        # TODO bundle parameters and their gradients together and return an iterable over them. \n",
    "        # for example:\n",
    "        return zip([self.parameterA, self.parameterA_grad], [self.parameterB, self.parameterB_grad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOO4lFZ_g7I9"
   },
   "source": [
    "Copy and paste your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oqOOhb2g7I_"
   },
   "outputs": [],
   "source": [
    "# TODO paste your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8mvj81Lg7JB"
   },
   "source": [
    "### Exercise: implement a loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBgcslmUg7JD"
   },
   "source": [
    "Here is the gradient descent code for our framework. \n",
    "**EXERCISE:** Implement one loss function and its derivate. I recommend mean-squared error (MSE).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWiWKjIHg7JI"
   },
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, net, batch_size, lr=0.05):\n",
    "        # reference to your neural network\n",
    "        self.net = net\n",
    "\n",
    "        # the number of samples in your training set, per epoch\n",
    "        self.N = batch_size\n",
    "\n",
    "        # the learning rate\n",
    "        self.lr = lr\n",
    "\n",
    "        # small number, might come in handy.\n",
    "        self.eps = np.finfo(float).eps\n",
    "\n",
    "    # send back the mean loss through the neural network\n",
    "    def backward(self, error):\n",
    "        self.net.backward(error / self.N)\n",
    "\n",
    "\n",
    "    # update any parameters with their respective gradients\n",
    "    def step(self):\n",
    "        for layer in self.net.net:\n",
    "            if layer.params:\n",
    "                for p, p_grad in layer.parameters():\n",
    "                    p -= self.lr * p_grad\n",
    "\n",
    "\n",
    "    # clear all the gradients for the next epoch\n",
    "    def zero_grad(self):\n",
    "        self.net.zero_grad()\n",
    "\n",
    "\n",
    "    # TODO fill out one loss function and its derivate\n",
    "    # mean squared error\n",
    "    def MSE(self, y, yhat):\n",
    "        pass\n",
    "\n",
    "    # gradient of mean squared error\n",
    "    def MSE_grad(self, y, yhat):\n",
    "        pass\n",
    "\n",
    "    # cross entropy\n",
    "    def CE(self, y, yhat):\n",
    "        pass\n",
    "\n",
    "    # gradient of cross entropy\n",
    "    def CE_grad(self, y, yhat):\n",
    "        pass\n",
    "\n",
    "    # logloss \n",
    "    def LogLoss(self, y, yhat):\n",
    "        pass\n",
    "\n",
    "    # gradient of logloss \n",
    "    def LogLoss_grad(self, y, yhat):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPzZbfYxg7JM"
   },
   "source": [
    "### Prepare data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kua7V0hhg7JO"
   },
   "source": [
    "We will use the iris dataset for our task. The labels will be converted to one-hot encoding. That means we will want our model to have an output vector of size 3 (Setosa, Versicolour, Virginica).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vmzxx-I0g7JP"
   },
   "outputs": [],
   "source": [
    "def prep_data():\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # load iris data, split and shuffle data sets\n",
    "    iris = load_iris()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        iris.data, iris.target, test_size=0.2, shuffle=True)\n",
    "\n",
    "    # Convert all data to numpy arrays\n",
    "    for x in chain(x_train, y_train, x_test, y_test):\n",
    "        x = np.array(x)\n",
    "\n",
    "    # Convert labels to one-hot representation\n",
    "    y_test = np.eye(3)[y_test]\n",
    "    y_train = np.eye(3)[y_train]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = prep_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Abr12usag7JV"
   },
   "source": [
    "### Exercise: Instantiate Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c7xcL54g7JX"
   },
   "source": [
    "Next we will instantiate a model. We will use linear layers for the trainable part. For activation functions you can try any, but the last layer should be either sigmoid, tanh, or softmax.  \n",
    "There is no right answer here, the architecture of the network is left open for you to select.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gg12N9h1g7JY"
   },
   "outputs": [],
   "source": [
    "class ournet(FFNN):\n",
    "    def __init__(self):\n",
    "        super(ournet, self).__init__()\n",
    "        self.net.append(linear())\n",
    "        self.net.append(sigmoid())\n",
    "        self.net.append(linear())\n",
    "        self.net.append(sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq0Z12xXg7Ja"
   },
   "source": [
    "### Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3SLU3ZVg7Jc"
   },
   "source": [
    "Now let's train our model and see how it does. You may need to play with the model architecture to get it right.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDAf2YnWg7Jd"
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "model = ournet()\n",
    "optimize = SGD(model, len(train_y), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIM0hdA9g7Jh"
   },
   "outputs": [],
   "source": [
    "t = trange(150)\n",
    "for epoch in t:\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        out = model.forward(x)\n",
    "        loss = optimize.MSE_grad(y, out)\n",
    "        optimize.backward(loss)\n",
    "\n",
    "    optimize.step()\n",
    "    optimize.zero_grad()\n",
    "\n",
    "    # shuffle data for next epoch\n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "\n",
    "    # Test the results for every epoch \n",
    "    correct = 0\n",
    "    for x, y in zip(x_test, y_test):\n",
    "        # Convert output to argmax\n",
    "        out = np.eye(3)[np.argmax(net.forward(x))]\n",
    "        if np.array_equal(y, out):\n",
    "            correct += 1\n",
    "\n",
    "    t.set_description(f\"(acc={correct/len(y_test)})\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQa08I-yg7Jm"
   },
   "source": [
    "Try some samples from the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxMq6Gckg7Jo"
   },
   "outputs": [],
   "source": [
    "def predict(data_index):\n",
    "    target_name = ({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "    out = np.argmax(net.forward(x_test[data_index]))\n",
    "    y_out = target_name[y_test[data_index]]\n",
    "    print(f\"idx: {data_index},  Prediction: {target_name[out]}   Actual: {y_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEXx9ADJg7Jr"
   },
   "outputs": [],
   "source": [
    "predict(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "AISC_Math_CodeChallenge3_std.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
