{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QU180tnf25N"
   },
   "source": [
    "## Hands-on Challenge Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lkDydpCf25N"
   },
   "source": [
    "Now that you have seen the different non-linear functions, it's time to implement them ourselves. We will implement each function as a layer for our MLP: \n",
    "\n",
    "1.  linearlayer\n",
    "2.  softmax\n",
    "3.  relu\n",
    "4.  sigmoid\n",
    "5.  tanh\n",
    "\n",
    "All the layers will have the same basic structure: \n",
    "\n",
    "-   initialize the required data structure(s)\n",
    "-   forward function that is called to process input data\n",
    "-   a reset function to clear out the outputs.\n",
    "\n",
    "**Note** There is a reason to have an *out* tensor and not just pass through the results. Later we will need the output results to compute the gradients. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "epdazNJQf25O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  \n",
    "import numpy as np\n",
    "\n",
    "class layer():\n",
    "    \n",
    "    #This is the layer superclass with only node_dim to specify\n",
    "    #This is the representation of just one layer\n",
    "    #we will use this as a base class (parent class) for linearlayer and non-linear functions.\n",
    "    \n",
    "    def __init__(self, node_dim):\n",
    "        \"\"\"\n",
    "        This init should be called via super() with the number\n",
    "        of nodes as an argument.\n",
    "        \"\"\"\n",
    "        self.input = np.zeros(node_dim)\n",
    "        self.input_grad = np.zeros(node_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input =  x\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.input_grad.fill(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nL-gcmfDf25T"
   },
   "source": [
    "However, the linearlayer adds two more arguments: the input dimension and a switch to include bias. Normally we want bias, but we will make it an option just in case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X8aAH3tWf25U"
   },
   "outputs": [],
   "source": [
    "class linearlayer(layer):\n",
    "    def __init__(self, in_dim, node_dim, bias=True):\n",
    "        super(linearlayer, self).__init__(node_dim)\n",
    "        self.out = np.zeros(node_dim)\n",
    "        self.weights = np.random.rand(node_dim,in_dim)\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = np.zeros(in_dim)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.input = x\n",
    "        self.out = np.dot(self.weights, x) #matrix multiplication of weights and inputs.\n",
    "        if self.bias.any():\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def reset(self):\n",
    "        self.out.fill(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-linear functions needs to be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu(layer):\n",
    "    def __init__(self, node_dim):\n",
    "        super(relu, self).__init__(node_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.clip(x, 0, None)\n",
    "     \n",
    "class softmax(layer):\n",
    "    def __init__(self, node_dim):\n",
    "        super(softmax, self).__init__(node_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        # return np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x))) -> as alternative\n",
    "        return (np.exp(x)) / (np.sum((np.exp(x))))\n",
    "    \n",
    "class sigmoid(layer):\n",
    "    def __init__(self, node_dim):\n",
    "        super(sigmoid, self).__init__(node_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return 1 / (1 + np.exp(-1 * x))\n",
    "    \n",
    "class tanh(layer):\n",
    "    def __init__(self, node_dim):\n",
    "        super(tanh, self).__init__(node_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return (np.exp(2*x) - 1) / (np.exp(2*x) + 1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5suH3vGKf25V"
   },
   "source": [
    "Go ahead and write the code for all the non-linearity layers. Then make a simple MLP, try and run it. \n",
    "\n",
    "\n",
    "Here's an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cw72U6gXf25X"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # The MLP will be a list with each layer as an item.\n",
    "        self.net = []\n",
    "\n",
    "        self.net.append(linearlayer(10, 20))\n",
    "        self.net.append(relu(20))\n",
    "        self.net.append(linearlayer(20, 4))\n",
    "        self.net.append(softmax(4))\n",
    "     \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Input x for each layer and return the result back into x,\n",
    "        # ready as input for the next layer.\n",
    "        for layer in self.net:\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        # traverse the MLP and call each layers 'reset' method\n",
    "        for layer in self.net:\n",
    "            layer.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6D7F_Qff25Y"
   },
   "source": [
    "Let's see if it works\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WwmDG89If25Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20412878 0.20541899 0.24846197 0.24222301 0.50406675 0.82262963\n",
      " 0.90324775 0.19573363 0.3249147  0.29344446]\n",
      "[0.61230688 0.00155266 0.15343245 0.23270802]\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "\n",
    "x = np.random.random(10) # inputs\n",
    "\n",
    "print(x)\n",
    "print(model.forward(x))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "AISC_Math_session1_challenge_sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
