{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0HviOhCvD9d"
   },
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "litde61tvD9g"
   },
   "source": [
    "This notebook is part of the workshop \"Mathematics of Deep Learning\" run\n",
    "by Aggregate Intellect Inc. ([https://ai.science](https://ai.science)), and is released\n",
    "under 'Creative Commons Attribution-NonCommercial-ShareAlike CC\n",
    "BY-NC-SA\" license. This material can be altered and distributed for\n",
    "non-commercial use with reference to Aggregate Intellect Inc. as the\n",
    "original owner, and any material generated from it must be released\n",
    "under similar terms.\n",
    "([https://creativecommons.org/licenses/by-nc-sa/4.0/](https://creativecommons.org/licenses/by-nc-sa/4.0/))\n",
    "\n",
    "In this notebook we will now look at the non-linear functions used throughout a neural network. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3Hync83vD9h"
   },
   "source": [
    "## Work with non-linearities:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "049CtFHnvD9h"
   },
   "source": [
    "For each non-linearity function, compute it's output on some input $x$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCz4ngwavD9h"
   },
   "source": [
    "### Affine map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_uDj1vavD9i"
   },
   "source": [
    "The basis of neural networks, the affine map is an abstraction of a neuron which multiplies each input by a weight and then sums them up. A bias is added to properly center the result.   \n",
    "\n",
    "Affine transformations are defined as \n",
    "\n",
    "$$ y = Ax + b $$\n",
    "\n",
    "where $A$ is a matrix of weights, $x$ is the input vector, and $b$ is a bias vector. It can be seen as a linear transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PozC2E6GvD9i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kqNNO1eJWLW",
    "outputId": "79a93731-2679-4d7d-f8ba-86e2ab9bf683"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3a400e5930>"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what is this?\n",
    "torch.manual_seed(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs76ZvHcJpoS"
   },
   "source": [
    "The matrix form of linear transformation for the data is defined as:\n",
    "\n",
    "\n",
    "$$y= xA^T +b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05x7vCrUy-9q",
    "outputId": "f46d1426-d3b2-4726-cda2-22455870539a"
   },
   "outputs": [],
   "source": [
    "# define an affine map where the input's size is 5 and output size is 3 (with nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_AwHZZi7ypMb",
    "outputId": "9df15ac0-e256-410b-c9df-f10b59758ec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519],\n",
      "        [-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]])\n"
     ]
    }
   ],
   "source": [
    "# define a 2x5 data\n",
    "data = torch.randn(2, 5)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQqtMWsoJ9PK"
   },
   "outputs": [],
   "source": [
    "# apply it to the data\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rE169qA0ypUy"
   },
   "outputs": [],
   "source": [
    "# what happened?\n",
    "# print the weights in lin.weight\n",
    "# print the bias in lin.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSMuHy_qKGBi"
   },
   "source": [
    "## Non-linear functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zto9io63vD9j"
   },
   "source": [
    "### Plot a rectified linear unit (ReLu) function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnNnVexhvD9k"
   },
   "source": [
    "The ReLu function is defined by:\n",
    "\n",
    "$$ y = \\begin{cases} \n",
    "x &; \\quad if \\quad x > 0 \\\\ \n",
    "0 &; \\quad otherwise \n",
    "\\end{cases} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMIE4yM5vD9l"
   },
   "outputs": [],
   "source": [
    "# define x to be a range of (-5,5)\n",
    "x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYnFDqbXv-4c"
   },
   "outputs": [],
   "source": [
    "# define y as relu(x) using torch.relu\n",
    "y = \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AT1P0UZHv_qE"
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96seiSJ0vD9l"
   },
   "source": [
    "### Plot a sigmoid function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxzgIFUYvD9m"
   },
   "source": [
    "Sigmoid function is defined as:\n",
    "\n",
    "$$ y = \\frac{1}{1 + e^{-x}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzDCHktSvD9n"
   },
   "outputs": [],
   "source": [
    "# define x to be a range of (-5,5)\n",
    "x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q--iMI_0wpDc"
   },
   "outputs": [],
   "source": [
    "# define y as sigmoid(x) using torch.sigmoid\n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iM_uV47MwpPr"
   },
   "outputs": [],
   "source": [
    "# plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7LWVLDWvD9o"
   },
   "source": [
    "### Plot a tanh function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ToS9VixvD9p"
   },
   "source": [
    "tanh function is defined as:\n",
    "\n",
    "$$ y = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8bOmCXLvD9p"
   },
   "outputs": [],
   "source": [
    "# define x to be a range of (-5,5)\n",
    "x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnXo4XUzwtcc"
   },
   "outputs": [],
   "source": [
    "# define y as tanh(x) using torch.tanh\n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_759ZpaiwuEs"
   },
   "outputs": [],
   "source": [
    "# plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oohvvDNvvD9p"
   },
   "source": [
    "### Define a ReLU layer in PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTuJdLfpvD9q",
    "outputId": "98d9adbc-efae-4c4b-c345-cec3ae460cc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3a400e5930>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEs_Y60JwyzV",
    "outputId": "3117fe08-451f-4a16-dc9d-f9aa0ad835e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1755, 0.0000, 0.0000],\n",
       "        [0.0000, 0.2260, 0.1089]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3) \n",
    "data = torch.randn(2, 5)\n",
    "torch.relu(lin(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faXBkFwdvD9q"
   },
   "source": [
    "Try and apply ReLU to the image we downloaded in the last notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6eeYvCZevD9q"
   },
   "outputs": [],
   "source": [
    "img = plt.imread('img.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvyjhj8cvD9q"
   },
   "source": [
    "### Experiment with softMax and verify it is a probability distribution function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qh2GKGyzvD9r"
   },
   "source": [
    "Softmax is defined as:\n",
    "\n",
    "$$ y = \\frac{e^{x_j}}{\\sum_j{e^{x_j}}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxO9Fuw_vD9r"
   },
   "outputs": [],
   "source": [
    "# define x to be a random tensor with the size of 5\n",
    "x = \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMa69OWFLFlq"
   },
   "outputs": [],
   "source": [
    "# apply softmax to x along the 0th dimension\n",
    "torch.softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXLoUTKbxX2k"
   },
   "outputs": [],
   "source": [
    "# verify that it returns a normalized probability distribution along that axis\n",
    "torch.softmax(x, dim=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBmRME_zxajS"
   },
   "outputs": [],
   "source": [
    "#take the output of the softmax, if this is a classification problem, what is the probability of the most probable class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fivyPPUFxe03"
   },
   "outputs": [],
   "source": [
    "# repeat with a 5x2 random tensor - remember to use .sum(0) instead of sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S_TGUp9vD9r"
   },
   "source": [
    "Why do we bother with exponents in softmax? Why not just normalize the output so that we have a probability distribution? \\\\\n",
    "The answer is that we want to mimic the *argmox* function; but a 'soft', probabilistic version of it.\n",
    "\n",
    "Let's say our model output vector is `[1, 1, 5, 3]` \\\\\n",
    "The argmax of the output is `[0, 0, 1, 0]`\n",
    "\n",
    "Let's compare a normalized version with softmax:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaEsAaUjvD9r"
   },
   "outputs": [],
   "source": [
    "logits = np.array([1, 1, 5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mMiyxdW1G0R"
   },
   "outputs": [],
   "source": [
    "# if we just normalized the output we would get\n",
    "print(logits/np.sum(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAi7SGrI1K-S"
   },
   "outputs": [],
   "source": [
    "# now lets look at the softmax version\n",
    "print(np.exp(logits)/np.sum(np.exp(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugXbosWDvD9s"
   },
   "source": [
    "We can see that softmax exaggerates the differences so that the maximum value is much greater than the other values. This is much closer to *argmax*, which is what we want.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZVonthAvD9s"
   },
   "source": [
    "## Hands-on Challenge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NygHrnl2vD9s"
   },
   "source": [
    "Now that you have seen the different non-linear functions, it's time to implement them ourselves. We will implement each function as a layer for our MLP: \n",
    "\n",
    "1.  linearlayer\n",
    "2.  softmax\n",
    "3.  relu\n",
    "4.  sigmoid\n",
    "5.  tanh\n",
    "\n",
    "All the layers will have the same basic structure: \n",
    "\n",
    "-   initialize the required data structure(s)\n",
    "-   forward function that is called to process input data\n",
    "-   a reset function to clear out the outputs.\n",
    "\n",
    "**Note** There is a reason to have an *out* tensor and not just pass through the results. Later we will need the output results to compute the gradients.\n",
    "\n",
    "The code below is Code Challenge 1 + non-linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MNatNtPvD9s"
   },
   "outputs": [],
   "source": [
    "class layer():\n",
    "    \n",
    "    #layer class with only node_dim to specify\n",
    "    #This is the representation of just one layer\n",
    "    #we will use this as a base class for linearlayer and non-linear functions.\n",
    "    \n",
    "    def __init__(self, node_dim):\n",
    "        \"\"\"\n",
    "        This init should be called via super() with the number\n",
    "        of nodes as an argument.\n",
    "        \"\"\"\n",
    "        #define basics that a layer would have: input, input_grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #define input as x\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        #clean your input_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAO6sIrlvD9t"
   },
   "source": [
    "However, the **linearlayer** adds two more arguments: the input dimension and a switch to include bias. Normally we want bias, but we will make it an option just in case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5SupOZuvD9t"
   },
   "outputs": [],
   "source": [
    "class linearlayer(layer):\n",
    "    # This could be a linear layer (with inputs and outputs)\n",
    "    def __init__(self, in_dim, node_dim, bias=True):\n",
    "         #You can inherit from your base class with super() builtin function.\n",
    "        self.out = #Initialize your output\n",
    "        self.weights = #Initialize your weights\n",
    "        if bias:\n",
    "            self.bias = #create tensor biases\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = np.zeros(in_dim)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #pass inputs and create your output (Remember W.X + b)\n",
    "        return #out\n",
    "    \n",
    "    def reset(self):\n",
    "       #clean outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdkBKiNtvD9t"
   },
   "source": [
    "Go ahead and write the code for all the non-linearity layers using numpy + maths formulas. Then append the layers on your simple MLP, try and run it.\n",
    "\n",
    "\n",
    "Here's a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu(layer):\n",
    "    def __init__(self, node_dim):\n",
    "        #Inheritate \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #return non-linear values.\n",
    "        return #np.clip(..)\n",
    "    \n",
    "class softmax(layer):\n",
    "    def __init__(self, node_dim):\n",
    "        #Inheritate \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #return non-linear values.\n",
    "        return #np.exp..\n",
    "    \n",
    "class sigmoid(layer):\n",
    "    def __init__(self, node_dim):\n",
    "        #Inheritate \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #return non-linear values.\n",
    "        return \n",
    "    \n",
    "class tanh(layer):\n",
    "    def __init__(self, node_dim):\n",
    "        #Inheritate \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #return non-linear values.\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taTXAMVDvD9t"
   },
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self):\n",
    "\n",
    "        # The MLP will be a list with each layer as an item.\n",
    "        self.net = []\n",
    "\n",
    "        self.net.append(linearlayer(10, 20))\n",
    "        self.net.append(relu(20))\n",
    "        self.net.append(linearlayer(20, 4))\n",
    "        self.net.append(softmax(4))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Input x for each layer and return the result back into x,\n",
    "        # ready as input for the next layer.\n",
    "        for layer in self.net:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        # traverse the MLP and call each layers 'reset' method\n",
    "        for layer in self.net:\n",
    "            layer.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKM7gEUSvD9u"
   },
   "source": [
    "Let's see if it works\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfEtc7CovD9u"
   },
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "\n",
    "x = np.random.random(10)\n",
    "\n",
    "print(x)\n",
    "print(model.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YL-CjXJpvD9u"
   },
   "source": [
    "Go ahead and try all the layers you made and see how they work.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "c_AISC_Math Session1_Book2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
