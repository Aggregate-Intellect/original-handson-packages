{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeS3hWphgwrS"
   },
   "source": [
    "# Sentence Classification with BERT\n",
    "\n",
    "In the last module, we learnt how to load a custom dataset, convert it into features, and divide the dataset into train/validation splits.\n",
    "In this module, we will learn how to fine-tune BERT for a sentence-level classification task. Using BERT, we will build a sentiment analyzer on the climate-change tweet dataset that we went through in the last notebook. \n",
    "\n",
    "__What you will learn:__\n",
    "This notebook, in addition to the learnings from the previous notebook, will help you finetune BERT for a sentence-level classification task. By the end of the notebook, you should have the skills to load a custom dataset, convert it into features, and finetune BERT for the task.\n",
    "\n",
    "Topics covered:\n",
    "- Sentence classification\n",
    "- Train/validation loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JscaMcpBgCkK"
   },
   "source": [
    "### Using GPUs on Colab\n",
    "To run this notebook on GPU, we will need to enable them on Colab. Enable GPUs by doing the following steps:\n",
    "- Click on `Edit`\n",
    "- `Notebook Settings`\n",
    "- Choose `GPU` under `Hardware accelerator`\n",
    "Wait until the resources have been allocated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrVX4lk9uQXS"
   },
   "source": [
    "### Recap\n",
    "In the last lesson, we learnt how to load our own dataset. Let's go ahead use that code to load a custom dataset. While we are at it, let's simplify the code into `convert_examples_to_features` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "executionInfo": {
     "elapsed": 50540,
     "status": "ok",
     "timestamp": 1606686513038,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "ijed7RxEhmOm",
    "outputId": "650a71ff-9803-4b60-d1d7-b5ac815d5336"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# climate_change_dataset = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wfv16DLohrjh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"twitter_sentiment_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5MsEmOmBegm"
   },
   "source": [
    "Set up `transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29814,
     "status": "ok",
     "timestamp": 1606686519331,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "Iiv8OLQMCHvV",
    "outputId": "e2640f25-ef79-4625-eaed-0063d37546e3"
   },
   "outputs": [],
   "source": [
    "# !pip install --quiet transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZQHnQEHCQJ3"
   },
   "source": [
    "### Load pretrained BERT\n",
    "We will now load BERT model and it's corresponding tokenizers. As you might already know, BERT is one of the widely used transformer architectures in NLP. To know more about BERT models, do checkout this great [video](https://ai.science/e/bertbert-explained-pre-training-of-deep-bidirectional-transformers-for-language-understanding--2018-11-06).\n",
    "\n",
    "Since our task is to do sequence or sentence-level classification, we will be using `AutoModelForSequenceClassification` module from transformers. `AutoClass` is just an abstraction that will automatically infer the transformer model type sparing the practitioners the pain of finding the right modules. Each `AutoClass` is mapped to individual model types. This model type is based on name of the model passed to the `AutoClass`. For example, if we were to pass `gpt2` to `AutoTokenizer`, we would automatically construct a `GPT2Tokenizer`. Read more about `AutoClass` [here](https://huggingface.co/transformers/model_doc/auto.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "2153a9e94a3249beb287ad34e8c194db",
      "f16b6f7677174889b4d62ceb989be643",
      "36cbc99dc1944bcc82233ca3b7aa8a8d",
      "bb58394181044854bf2e292fa1448222",
      "6382f5f481a2420d903255404fe927ac",
      "57ba9ce8d8924cc6b7b3d3798995cac6",
      "508df94bc3b745ae90f16ea3055d5df2",
      "f619d406b2774425aa67c0cb04d08696",
      "6c8dff40375f44d9b0707bc5b9d6091e",
      "126d4d457e8d4b4d97081bc1c7dc4f4d",
      "0e8ed57eb1244192b66e0f1c6bade4b8",
      "eb9ed820f2e14c6ab0ae877531564ca1",
      "3c7d825cf0ff4b2fa720e0295d6e3ab1",
      "6a0df2cb5c9f4f618064a696c153150e",
      "dab24051f83a4133aed1467dffbb72a7",
      "740bcae90c2548b3aae11801eff59a8b"
     ]
    },
    "executionInfo": {
     "elapsed": 35034,
     "status": "ok",
     "timestamp": 1606686525531,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "K3arEDIlBanN",
    "outputId": "30387055-50f6-4bf4-899b-0c104915e87e"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plc04MZDDBWA"
   },
   "source": [
    "# Pre-process data\n",
    "Here comes our pre-processing step from the last notebook. One more step that we will also add in this notebook is  to convert the labels to corresponding numerical forms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zSVumBwZuqCP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "MAX_LEN = 64\n",
    "label2id = {id:id+1 for id in range(-1, 3, 1)}\n",
    "id2label = {v:k for k, v in label2id.items()}\n",
    "\n",
    "def convert_examples_to_features(tweets, labels):\n",
    "  input_ids = [\n",
    "      bert_tokenizer.encode(tweet, add_special_tokens=True) for tweet in tweets\n",
    "  ]\n",
    "\n",
    "  input_ids = pad_sequences(\n",
    "      input_ids,\n",
    "      maxlen=MAX_LEN,\n",
    "      dtype=\"long\", \n",
    "      value=bert_tokenizer.pad_token_id,\n",
    "      padding=\"post\",\n",
    "      truncating=\"post\"\n",
    "  )\n",
    "\n",
    "  input_ids = torch.tensor(input_ids)\n",
    "  attention_masks = (input_ids > 0).int()\n",
    "  labels = torch.tensor([label2id[label] for label in labels])\n",
    "\n",
    "  return TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hK6hRHWRv5Uc"
   },
   "outputs": [],
   "source": [
    "dataset = convert_examples_to_features(df.message, list(df.sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR4YCheYFQUJ"
   },
   "source": [
    "### Train/Validation Set\n",
    "Divide the dataset into train/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59353,
     "status": "ok",
     "timestamp": 1606686552535,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "tkMAVUZhv2xU",
    "outputId": "0aab9042-293e-4f6f-ad3a-d356ef1c8071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 35154, Validation size: 8789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    dataset,\n",
    "    list(df.sentiment), \n",
    "    random_state=1234,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxGN3YnHFvhn"
   },
   "source": [
    "# Model definition\n",
    "Okay, let's get right into the defining our model. We will be loading the `bert-base-uncased` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "referenced_widgets": [
      "27c1b8278b644b8d93dcf18f514a9909",
      "40507962b8224d8b992b0b53e368da89",
      "1e046dc129184149b2defaf7f134904f",
      "ac6390775ba044c4b3fe685734041d8e",
      "f1976bbed4554821b9cbfcdac8a47965",
      "930f24c442624a47b68c69bcac190d77",
      "71b09d5743c84ada975295ada6057134",
      "b5b9f0a894eb41b28455df066e386767"
     ]
    },
    "executionInfo": {
     "elapsed": 68252,
     "status": "ok",
     "timestamp": 1606686562153,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "1w4eaq9DFuU4",
    "outputId": "329e43a3-6717-4eeb-fc96-88569d373c63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH7LTlxJV0Gy"
   },
   "source": [
    "Using `AutoModelForSequenceClassification` will map to `BertForSequenceClassification` as defined [here](https://github.com/huggingface/transformers/blob/c89bdfbe720bc8f41c7dc6db5473a2cb0955f224/src/transformers/models/bert/modeling_bert.py#L1313). When you open the link, you'll see that the `BertForSequenceClassification` model essentially does the following:\n",
    "- `BERT` model\n",
    "- Dropout\n",
    "- a Linear layer\n",
    "\n",
    "Note that `BertForSequenceClassification` already comes with a linear layer, and we just have to modify it to meet our requirements. BERT output for each token (`MAX_LEN` number of tokens) is a 768-dimension vector. The BERT vector corresponding to the `CLS` token is used for sequence classification tasks. The linear layer maps the 768-dimension vector to a vector of output-size dimension.\n",
    "\n",
    "You might also notice that the final number of labels is determined by a config file. Let us load the config file corresponding to the pre-trained model and investigate whether the `num_labels` is equal to 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67328,
     "status": "ok",
     "timestamp": 1606686562154,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "aRryi6psZK_H",
    "outputId": "909d3124-a859-44f2-951b-23c5b6f7c580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "bert_config = AutoConfig.from_pretrained(\n",
    "    \"bert-base-uncased\"\n",
    ")\n",
    "print(bert_config.num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zHMdExVY-U0"
   },
   "source": [
    "The default number of output labels in the pre-trained model is 2. But, our classification task has 4 labels. Let us fix that first. To do this obtian the pre-trained model config and change the number of labels to 4. We will also have to specify the new label to id mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_BTU_LLMYR5u"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "bert_sequential_config = AutoConfig.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=4,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69775,
     "status": "ok",
     "timestamp": 1606686566176,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "FghRbaUXc1bp",
    "outputId": "52aa98dc-0897-4ab2-8212-e93377dcc597"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_sequential_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            pretrained_model_name_or_path=\"bert-base-uncased\",\n",
    "            config=bert_sequential_config,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KymYDed6fppz"
   },
   "source": [
    "That is everything! We will now model to GPU using `.to()` if `GPU`s are available. If `GPU`s is available, we set the device to be `cuda`, `cpu` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82092,
     "status": "ok",
     "timestamp": 1606686580530,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "9aULGJmzfk5n",
    "outputId": "4dcadc6a-44d7-40dd-be0d-6ef2dadbf7e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Moving model to device: {device}\")\n",
    "bert_sequential_model = bert_sequential_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuOM3ZA3hjXo"
   },
   "source": [
    "# Training\n",
    "Alright, let's revisit what we have done so far:\n",
    "1. Preprocess data: done!\n",
    "2. Load pre-trained model: done!\n",
    "3. Train model: let's get to it now!\n",
    "\n",
    "In this step, we will sample data in batches from our train `DataLoader` and will finetune our BERT model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXdpuRo_izQQ"
   },
   "source": [
    "## Setup Dataloaders\n",
    "\n",
    "Time to answer the homework question from last notebook. The question was why shouldn't we use a `RandomSampler` on the validation dataset. It is not a good practice to use `RandomSampler` because with random sampling, the validation accuracy will vary and therefore the model that is saved based on the best validation accuracy. Avoid using random sampler and use a `SequentialSampler` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ERqvVES_ib2d"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    TensorDataset,\n",
    "    RandomSampler,\n",
    "    SequentialSampler,\n",
    ")\n",
    "\n",
    "# BATCH_SZ = 64\n",
    "BATCH_SZ = 2\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=BATCH_SZ\n",
    ")\n",
    "\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_data,\n",
    "    sampler=val_sampler,\n",
    "    batch_size=BATCH_SZ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEB-v3tHi3D7"
   },
   "source": [
    "## Setup optimizer\n",
    "As a part of the training process, we should also define an optimizer. We will use our good old `SGD` optimizer in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7Qy_37ddiwNU"
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# define a learning rate\n",
    "LR=5e-4\n",
    "optimizer = SGD(bert_sequential_model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srGrDjTjjhch"
   },
   "source": [
    "## Training loop\n",
    "\n",
    "The training loop has two steps:\n",
    "1. Epoch loop: an epoch means one pass through the training dataset. \n",
    "2. Batch training loop: in this inner loop, the model is trained on a batch of data at each step.\n",
    "\n",
    "At the end of each epoch, we run our model through the validation dataset and calculate evaluation metrics (accuracy and f1) on it. As you might know, at the beginning of each training loop, we have to specify that we're in training mode by using `model.train()` method. This is to ensure that opertions like `dropout` and `batchnorm` are performed in train mode rather than in evaluation mode. Similarly, during the validation phase, we make sure that gradients are not computed, which will help in computation speedup. \n",
    "\n",
    "Some of these concepts can be confusing and you might ask, \"oh what if I accidentally forget to add `model.train()`\". Luckily for us, we have [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) which will abstract away a lot of these boiler plate code as we will see in the next notebook.\n",
    "\n",
    "Before the training process:\n",
    "- BERT model has weights from pre-training\n",
    "- the linear model on top has random weights\n",
    "\n",
    "During finetuning process:\n",
    "- the weights of the linear layer as well as the model will be updated. \n",
    "- we can also control what layers in the overall model should be updated. For example, we can only decide to finetune the linear layers, or certain layers in BERT model. For the purpose of our notebook, we will finetune the entire model (BERT + linear layers) for 7 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1865150,
     "status": "ok",
     "timestamp": 1606690152153,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "qdsGSGC0jb9C",
    "outputId": "e73cb6a7-3770-4652-f1d3-c0d7835729cd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "EPOCHS = 7\n",
    "loss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    batch_loss = 0\n",
    "    # The model is in training model now; while in evaluation mode,\n",
    "    # we change this to .eval()\n",
    "    bert_sequential_model.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # move the input data to device\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        # pass the input to the model\n",
    "        outputs = bert_sequential_model(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        # set model gradients to 0, so that optmizer won't accumulate\n",
    "        # them over subsequent training iterations\n",
    "        optimizer.zero_grad()\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # obtain loss, and backprop\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        #clip gradient norms to avoid any exploding gradient problems\n",
    "        # torch.nn.utils.clip_grad_norm_(bert_sequential_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_train_loss = batch_loss / len(train_dataloader)  \n",
    "    print(f\"epoch: {epoch+1}, train_loss: {epoch_train_loss}\")\n",
    "    \n",
    "    # At the end of each epoch, we will also run the model \n",
    "    # on the validation dataset\n",
    "    val_loss, val_accuracy = 0, 0\n",
    "    true_labels, predictions = [], []\n",
    "\n",
    "    for val_batch in val_dataloader:\n",
    "        val_batch = tuple(t.to(device) for t in val_batch)\n",
    "        input_ids, attention_mask, labels = val_batch\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = bert_sequential_model(\n",
    "              input_ids, \n",
    "              attention_mask=attention_mask, \n",
    "              labels=labels\n",
    "            )\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        # convert predictions and gold labels to numpy arrays so that\n",
    "        # we can compute evaluation metrics like accuracy and f1\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        preds = outputs[1].detach().cpu().numpy()\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        true_labels.extend(label_ids)\n",
    "        predictions.extend(preds)\n",
    "      \n",
    "    acc = f1_score(y_true=true_labels, y_pred=predictions, average='micro')\n",
    "    f1 = f1_score(y_true=true_labels, y_pred=predictions, average='macro')\n",
    "\n",
    "    print(f\"epoch: {epoch+1} val loss: {val_loss}, accuracy:{acc}, f1:{f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuFi13d8htSM"
   },
   "source": [
    "## On Data Annotation\n",
    "In an industrial setting, you may not have enough data to even support finetuning. In such cases, it is general practice to annotate more data. Some standards steps that you would follow as a Machine Larning practioner for faster data annotation:\n",
    "1. Collect unlabeled data relevant for the downstream task\n",
    "2. Label the dataset with a pre-trained model\n",
    "3. Manually verify the label, annotate in cases where there are wrong predictions.\n",
    "\n",
    "[Prodigy](prodi.gy) is an annotation tool that is widely used in industry for annotation purposes. \n",
    "One of the advantages of using Prodigy makes annotation simpler and faster through its integrations with existing models! Our team has labelled thousands of samples just in a matter of hours by leveraging existing models + Prodigy.  For a quick look on how Prodigy looks like and its working, check out this [video](https://www.youtube.com/watch?v=5di0KlKl0fE) by their co-founder Inas. \n",
    "\n",
    "\n",
    "If you are an organization with multiple annotators, it is totally worth the buy, otherwise it is way too expensive!  You can install prodigy by following the instructions from [here](https://prodi.gy/docs/install).  If Prodigy seems too expensive for your needs, consider annotating with a script or through Jupyter Notebook.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6jmT8hmfpdw"
   },
   "source": [
    "# Homework: Custom Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duM1lOZefYi0"
   },
   "source": [
    "So far, we have used the `Linear` layer provided by `BertForSequenceClassification` class. As a part of your homework, construct a custom neural network to add a multi-layer-perceptron (MLP) on top of `BERT` model. \n",
    "\n",
    "Some hints:\n",
    "- Define a PyTorch model by inheriting `nn.Module` class\n",
    "- Use `AutoModel` to obtain an instance of `BertModel`\n",
    "    ```python\n",
    "    bert_model = Automodel.from_pretrained(\n",
    "          pretrained_model_name_or_path=\"bert-base-uncased\"\n",
    "    )\n",
    "    ```\n",
    "- Add `MLP` layers on top:\n",
    "  - BERT output vector dimension is 768, therefore the input dimension of MLP should also be 768\n",
    "\n",
    "This excercise might tuurn out to be a bit challenging, but don't worry as we will learn how to define a custom network in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdGA4wjUhVPK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "L03_Sentence_Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e8ed57eb1244192b66e0f1c6bade4b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a0df2cb5c9f4f618064a696c153150e",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c7d825cf0ff4b2fa720e0295d6e3ab1",
      "value": 231508
     }
    },
    "126d4d457e8d4b4d97081bc1c7dc4f4d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e046dc129184149b2defaf7f134904f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_930f24c442624a47b68c69bcac190d77",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f1976bbed4554821b9cbfcdac8a47965",
      "value": 440473133
     }
    },
    "2153a9e94a3249beb287ad34e8c194db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_36cbc99dc1944bcc82233ca3b7aa8a8d",
       "IPY_MODEL_bb58394181044854bf2e292fa1448222"
      ],
      "layout": "IPY_MODEL_f16b6f7677174889b4d62ceb989be643"
     }
    },
    "27c1b8278b644b8d93dcf18f514a9909": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1e046dc129184149b2defaf7f134904f",
       "IPY_MODEL_ac6390775ba044c4b3fe685734041d8e"
      ],
      "layout": "IPY_MODEL_40507962b8224d8b992b0b53e368da89"
     }
    },
    "36cbc99dc1944bcc82233ca3b7aa8a8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57ba9ce8d8924cc6b7b3d3798995cac6",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6382f5f481a2420d903255404fe927ac",
      "value": 433
     }
    },
    "3c7d825cf0ff4b2fa720e0295d6e3ab1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "40507962b8224d8b992b0b53e368da89": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "508df94bc3b745ae90f16ea3055d5df2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57ba9ce8d8924cc6b7b3d3798995cac6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6382f5f481a2420d903255404fe927ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6a0df2cb5c9f4f618064a696c153150e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c8dff40375f44d9b0707bc5b9d6091e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0e8ed57eb1244192b66e0f1c6bade4b8",
       "IPY_MODEL_eb9ed820f2e14c6ab0ae877531564ca1"
      ],
      "layout": "IPY_MODEL_126d4d457e8d4b4d97081bc1c7dc4f4d"
     }
    },
    "71b09d5743c84ada975295ada6057134": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "740bcae90c2548b3aae11801eff59a8b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "930f24c442624a47b68c69bcac190d77": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac6390775ba044c4b3fe685734041d8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5b9f0a894eb41b28455df066e386767",
      "placeholder": "",
      "style": "IPY_MODEL_71b09d5743c84ada975295ada6057134",
      "value": " 440M/440M [00:06&lt;00:00, 72.2MB/s]"
     }
    },
    "b5b9f0a894eb41b28455df066e386767": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb58394181044854bf2e292fa1448222": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f619d406b2774425aa67c0cb04d08696",
      "placeholder": "",
      "style": "IPY_MODEL_508df94bc3b745ae90f16ea3055d5df2",
      "value": " 433/433 [00:00&lt;00:00, 11.6kB/s]"
     }
    },
    "dab24051f83a4133aed1467dffbb72a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb9ed820f2e14c6ab0ae877531564ca1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_740bcae90c2548b3aae11801eff59a8b",
      "placeholder": "",
      "style": "IPY_MODEL_dab24051f83a4133aed1467dffbb72a7",
      "value": " 232k/232k [00:00&lt;00:00, 2.22MB/s]"
     }
    },
    "f16b6f7677174889b4d62ceb989be643": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1976bbed4554821b9cbfcdac8a47965": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f619d406b2774425aa67c0cb04d08696": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
