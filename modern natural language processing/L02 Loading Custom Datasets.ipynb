{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KHNjatRzoIh"
   },
   "source": [
    "# Transfer Learning\n",
    "In the previous module, we learnt how to use existing HuggingFace models using `pipeline` API and `AutoClass` features.  In this module, we will make progress towards tailoring existing models for our usecases through *transfer learning*. We will first go over pretraining and finetuning concepts. You will then learn how to load your own dataset using PyTorchâ€™s `TensorDataset`, `DataLoader`. In the next notebook,  we use the loaded custom dataset for building models.\n",
    "\n",
    "__What you will learn:__\n",
    "\n",
    "By the end of this notebook, you will have the skills to load your own dataset, tokenize, and divide it into train, valid, and test dataset.\n",
    "\n",
    "The notebook is divided into 2 sections. In the first section, we will discuss pretraining and finetuning concepts. We will learn how to load some of the pretrained transformer models. In the second section, we will learn data preparation---how to load custom datasets to be processed by transformers. \n",
    "\n",
    "Topics covered:\n",
    "- Pretraining and Finetuning\n",
    "- Datasets and Dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HG--Oa56z1fn"
   },
   "source": [
    "### Pretraining, Finetuning and Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_c3Hfex3SUA"
   },
   "source": [
    "Let's say your company wants to build the following products over 2020:\n",
    "- sentimental analyzer (Jan - Mar, 2020)\n",
    "- paraphrase detecter (Apr - Jun, 2020)\n",
    "- named entity recognizer (Jul - Sep, 2020)\n",
    "- quesion answering system (Oct - Dec, 2020)\n",
    "\n",
    "The company is willing to provide about 5000 training examples for each of the tasks. You, as a Machine Learning practitioner will start building neural networks for each of these tasks over the year. Your individual classifiers might be randomly initialized or initialized with word embeddings. As you are building more and more classifiers, you realize that the learnings from one task could potentially help another task. In some ways, the learning from one task can be \"transferred\" to another task. Even better, what if we could build one model that can be modified for different tasks?\n",
    "\n",
    "The ML community has built several such pretrained models on large amount of data. Since labeling is expensive, the tasks on which these large models are trained, are self-supervised in nature. For example, during pretraining  of BERT model, it learns to predict the masked words in a given sentence. \n",
    "\n",
    "```\n",
    "Amazon is the longest [MASK] in the [MASK]\n",
    "```\n",
    "\n",
    "By learning to predict that `MASK` here is `{river, world}`, BERT learns *contextual word embeddings*. BERT was trained on millions of such examples, To be specific, the pretraining was performed on BookCorpus data (800M words) and English Wikipedia (2500M words), and the models were made publicly available by Google.\n",
    "\n",
    "#### Glossary\n",
    "- __downstream tasks__: \n",
    "\n",
    "Downstream tasks are end-tasks that are of interest to you and to your company. Tasks like sentiment analysis, question answering etc., that have limited training examples are downstream tasks. \n",
    "\n",
    "- __transfer learning__:\n",
    "\n",
    "While training  downstream tasks, instead of randomly initializing the network, we would rather initialize it with weights correspodning to the pretrained model. The pretrained models, having gone through millions of words in gigantic corpora, can produce better representation for words in our input. By initializing a model with pretrained model weights, we can work on a downstream task with fewer labels. This is called *transfer learning*, where knowledge from the pretrained task is said to be \"transferred\" to the downstream task.\n",
    "\n",
    "- __fine tuning__:\n",
    "\n",
    "Finetuning is simply training a downstream task by initializing the network with weights corresponding to a pretrained model.\n",
    "\n",
    "Okay, enough theory, let's load some pretrained models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_t5OrK23TAj"
   },
   "outputs": [],
   "source": [
    "# !pip install --quiet transformers\n",
    "#or if you are using conda\n",
    "# conda install -c huggingface transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCHDEgttFT2_"
   },
   "source": [
    "From our previous notebook, you might remember that the `pipeline` API will run a query against a pretrained model. Therefore, it cannot be used for finetuning a downstream task. \n",
    "\n",
    "The two packages that will be handy are:\n",
    "- `AutoModel`: obtain models like GPT-2, ProphetNet, BART etc\n",
    "- `AutoTokenizer`: helps us get the tokenizers that were originally used for tokenizing text for these pretrained models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rHa20l2C0M8Z"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4YYbogfJH7k"
   },
   "source": [
    "To obtain pretrained models, simply use the function `from_pretrained()` and pass the model path to `pretrained_model_name_or_path` parameter. Go through the [model repository](https://huggingface.co/models) to try different pretrained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CmahYaCW0PZW"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a5e9e254f84cf0ba5f9f00069b1d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7706871a92924a77bb88073bd90680c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115f474dc9b84b54b71c9e767b0f2a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83ad5ffb9254963969e0d16163a6d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8fa54776924a01b88403da298be0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_model = AutoModel.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"bert-base-uncased\"\n",
    ")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"bert-base-uncased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27556,
     "status": "ok",
     "timestamp": 1606523736707,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "GOu21CzJR_9B",
    "outputId": "5f615c3d-cc41-4f11-c49e-b66765663791"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4KkCmrVKBGC"
   },
   "source": [
    "Let's look at another example. How about [T5](https://arxiv.org/pdf/1910.10683.pdf), another transformer-based model? T5 is a transformer model that is trained on a large pretraining dataset called [Colossal Clean Crawled Corpus (C4)](https://commoncrawl.org/). T5 also has more parameters and outperforms BERT on several downstream tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30843,
     "status": "ok",
     "timestamp": 1606523740001,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "G2L4Rtl9KAS7",
    "outputId": "9012ed03-9bbe-47db-a6a0-c5a09f5b9f2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:810: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fac39b4d95b4f51a968a20c4b362eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025c5a6ec1ad458cb7d13139995c6932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c190734221a4676954765a0c95e6688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b2549842cb470cab6a8bfcc7363fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead\n",
    "\n",
    "t5_model = AutoModelWithLMHead.from_pretrained(pretrained_model_name_or_path=\"t5-small\")\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmmZOJsqsNpI"
   },
   "source": [
    "Before an input text is passed to the model, it has to be converted into numerical representation. Tokenizer's `encode` method helps us accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fwzcZ5DXLCOc"
   },
   "outputs": [],
   "source": [
    "query = \"I think it is going to rain tonight\"\n",
    "tokenized_query = bert_tokenizer.encode(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQw21gfEsfRA"
   },
   "source": [
    "Let's take a look at what the numerical representation of the input sentence looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30837,
     "status": "ok",
     "timestamp": 1606523740004,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "hzGMGON9Lro4",
    "outputId": "d3f24f05-e059-4533-e93a-be7dc400f158"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 2228, 2009, 2003, 2183, 2000, 4542, 3892, 102]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZp0MezeOTFB"
   },
   "source": [
    "The numericalized tokens will then be passed to the model along with information like attention mask as we shall soon see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ey0NSSx0Uas"
   },
   "source": [
    "# Loading your own datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gMicYn4L0ai0"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3n_qyT-0boG"
   },
   "source": [
    "Now, one may not have huge amount of data, enough for even finetuning tasks. As a Machine Learning practitioner, you will often run into such scenarios. Trying to find more labeled data on the Internet, might turn out to be a wild goose chase. An alternative solution is to annotate your own dataset! Yes, you heard that right!\n",
    "\n",
    "There are some really cool tools such as [prodigy](https://prodi.gy/) that can help us annotate faster. We will learn more about prodigy and how we can use it to augment our dataset in our next notebook. For now, we will use the `Dataset Search` app from Google to obtain a dataset. \n",
    "\n",
    "Head over to [Dataset Search](https://datasetsearch.research.google.com/), and search for `Twitter Climate Change Sentiment Dataset`, and download the dataset from Kaggle, and unzip it on your local computer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9L-dBcl1S2ne"
   },
   "source": [
    "Let's upload the dataset here. We will be using the same dataset in notebooks 3 and 4 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-colab\n",
    "# # or\n",
    "# !conda install -c conda-forge google-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "executionInfo": {
     "elapsed": 79897,
     "status": "ok",
     "timestamp": 1606523789071,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "EUU8rtDHS8UM",
    "outputId": "6453544a-ddc8-4173-87b3-00083fe875fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/google/colab/data_table.py:30: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils import traitlets as _traitlets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-3d9a68b7-ca8d-443a-a255-5eeca0b45c59\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-3d9a68b7-ca8d-443a-a255-5eeca0b45c59\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8c7f65a7f5cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclimate_change_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "climate_change_dataset = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXsW9i1qvvtm"
   },
   "source": [
    "Let's load the dataset into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nahsufY9krGe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"twitter_sentiment_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI_THezZwVpX"
   },
   "source": [
    "It's always a good idea to take a look at what the dataset looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzHCQ3ktwvw4"
   },
   "source": [
    "The dataset has 4 classes: `{-1, 0, 1, 2}`. `-1` indicates the most negative \n",
    "outlook towards climate change, and `2` implies the most positive outlook. Our task, therefore, is to identify the sentiment for a given tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWlnhfyUypjS"
   },
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttozc7sAyBNp"
   },
   "source": [
    "As we discussed earlier in this notebook, our first step is to map the tokens into a consistent numerical representation using tokenizers. For the rest of the learning package, we will work with BERT models as they are the most popular NLP model in the recent past.\n",
    "\n",
    "There are a few special tokens in BERT:\n",
    "1. __[CLS]__: this is a special token that is added at the beginning of every sequence. The final hidden vector correspoding to this token is used for classification tasks.\n",
    "2. __[SEP]__: this token sepearates two sequences\n",
    "3. __[PAD]__: pad the sequence tokens upto `MAX_LEN` with a pad token (`0`), where MAX_LEN is the maximum length of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRa5ijsizJQa"
   },
   "source": [
    "Using the `encoder` method, we will first convert input tokens to their corresponding numerical form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1re-JA270m4Y"
   },
   "source": [
    "### Encoding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kKZO9XDo8gB7"
   },
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "\n",
    "for tweet in df.message:\n",
    "    tweet_in_ids = bert_tokenizer.encode(tweet, add_special_tokens=True)\n",
    "    input_ids.append(tweet_in_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRWrVo-nzw-y"
   },
   "source": [
    "The `add_special_tokens` parameter automatically adds `CLS` and `SEP` token at the beginning and at the end of the sentnce respectively. Let's take a look at what the input tokens look like after they have been numericalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26915,
     "status": "ok",
     "timestamp": 1606524384078,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "ynHnBz1LUjPR",
    "outputId": "1e34c68f-3e26-49c6-f240-f9eba9b1aec3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1030,\n",
       " 9543,\n",
       " 2666,\n",
       " 4783,\n",
       " 19092,\n",
       " 4785,\n",
       " 2689,\n",
       " 2003,\n",
       " 2019,\n",
       " 5875,\n",
       " 15876,\n",
       " 22516,\n",
       " 2004,\n",
       " 2009,\n",
       " 2001,\n",
       " 3795,\n",
       " 12959,\n",
       " 2021,\n",
       " 1996,\n",
       " 4774,\n",
       " 3030,\n",
       " 12959,\n",
       " 2005,\n",
       " 2321,\n",
       " 2748,\n",
       " 2096,\n",
       " 1996,\n",
       " 15620,\n",
       " 8797,\n",
       " 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C3otFOyguZx"
   },
   "source": [
    "The maximum token (sequence) length supported by BERT is 512. But, our task doesn't always need so many tokens. If our `MAX_LEN` is 64, we would want the rest of the tokens to be filled with a `PAD` token. We can do this with keras's `pad_sequences` library.\n",
    "\n",
    "Also, since the model eventually expects all the variables to be tensors, let us go ahead convert all our variables to tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4V85x8k-0rmR"
   },
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras\n",
    "# # or \n",
    "# !conda install -c conda-forge keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IWmdLj5F9SgS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kbak/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 64\n",
    "PAD_TOKEN = 0\n",
    "input_ids = pad_sequences(\n",
    "    input_ids,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype=\"long\", \n",
    "    value=bert_tokenizer.pad_token_id,\n",
    "    padding=\"post\",\n",
    "    truncating=\"post\"\n",
    ")\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "# Warnings below are due to the newer numpy version w/TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1606524484429,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "ECfQQc8_T_DS",
    "outputId": "96f86fd6-29b1-4e7c-f12b-d1a08cca68d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1030,  9543,  2666,  4783, 19092,  4785,  2689,  2003,  2019,\n",
       "         5875, 15876, 22516,  2004,  2009,  2001,  3795, 12959,  2021,  1996,\n",
       "         4774,  3030, 12959,  2005,  2321,  2748,  2096,  1996, 15620,  8797,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xO_VHfylh_GL"
   },
   "source": [
    "let's take a look at how the first sentence looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108167,
     "status": "ok",
     "timestamp": 1606523817358,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "p7YgFeEjUCCA",
    "outputId": "bf3325cd-a031-4209-fb87-12da9a02ecc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1030,  9543,  2666,  4783, 19092,  4785,  2689,  2003,  2019,\n",
       "         5875, 15876, 22516,  2004,  2009,  2001,  3795, 12959,  2021,  1996,\n",
       "         4774,  3030, 12959,  2005,  2321,  2748,  2096,  1996, 15620,  8797,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ_g7HtpiHR2"
   },
   "source": [
    "As you can see, the input_ids are now padded with the `PAD` token 0. The `post` option in padding implies that the `PAD` token will be appended instead of prepending ot the list. Similarly, `post` in `truncating` ensures that all extra tokens beyond `MAX_LEN` will be removed at the end of the list rather than the beginning of the list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQwZ54vE0vdF"
   },
   "source": [
    "### Attention mask\n",
    "With attention mask we let BERT know which tokens should be attended to,**bold text** and which should not. All the actual tokens will have an attention mask `1` and the rest of the tokens (pad tokens) will have mask `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZADro-23WE6o"
   },
   "outputs": [],
   "source": [
    "attention_masks = torch.tensor([[int(tok > 0) for tok in tweet] for tweet in input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32234,
     "status": "ok",
     "timestamp": 1606523918699,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "K_U6NTKER1JE",
    "outputId": "d56fae86-18c1-444c-cc96-7ff5e9a67b74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq87eEmbRBpp"
   },
   "source": [
    "which can also be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1Puufjk9Q-DN"
   },
   "outputs": [],
   "source": [
    "attention_masks = (input_ids > 0).int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBll7ZuA1F_C"
   },
   "source": [
    "Here's how the attention mask corresponding to our first sentence looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26292,
     "status": "ok",
     "timestamp": 1606523918701,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "As29EhIwpkg_",
    "outputId": "02e43982-b017-45bb-c3d1-0a32cdb4c4fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_l129_T1OCj"
   },
   "source": [
    "# Constructing Train, Validation Dataset\n",
    "The Twitter dataset on climate change does not come with a train/dev/test split. Let's manually split the dataset using `scikit-learn`'s `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GIyd2smaW_Up"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    input_ids,\n",
    "    list(df.sentiment), \n",
    "    random_state=1234,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "train_mask, val_mask, _, _ = train_test_split(\n",
    "    attention_masks,\n",
    "    list(df.sentiment),\n",
    "    random_state=1234,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "#let's also convert labels to tensors\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbhP1mq4yN8s"
   },
   "source": [
    "Awesomesauce! We now have split the dataset into `train`, `validation` sets. We have also managed to map text to ids using BERT tokenizer. The final step is to construct a `DataLoader` so that we can support sampling and batch training on our dataset. \n",
    "\n",
    "Since the dataset is already in tensor format, we will use `TensorDataset` to wrap it into a `DataSet` before constructing a `DataLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "J1AuGhfvoBEF"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "BATCH_SZ = 4\n",
    "train_dataset = TensorDataset(train_data, train_mask, train_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=BATCH_SZ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Hb-EfdfaAbV"
   },
   "source": [
    "Let us take the first batch of the dataloader and see how the data looks like. You should be able to see the four samples retrieved from training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108299,
     "status": "ok",
     "timestamp": 1606523817507,
     "user": {
      "displayName": "Royal Sequeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjwweNvsOAMOqQVZkNmtUdoCqD9Yhnq4x9_tGBEOQ=s64",
      "userId": "13091047729773909410"
     },
     "user_tz": 300
    },
    "id": "Bd3X8GkCq-2g",
    "outputId": "45f297eb-2e80-4f9a-b29f-89103a05666f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  101, 19387,  1030,  2197, 28075,  2669, 18743,  1024,  2065,  2017,\n",
       "          24260, 29645,  2050,  1522,  1078,  2050,  1525,  1068,  2102,  2903,\n",
       "           2158,  1011,  2081,  3795, 12959,  2003,  1037,  1037, 10021,  3277,\n",
       "           1010,  2507,  2000,  1996,  3019,  4219,  3639,  2473,  1006, 16770,\n",
       "           1024,  1037, 29645,  2050,  1522,  1078,  2050, 29649,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101, 19044,  2708,  1024,  6351, 14384,  2025,  1005,  3078, 12130,\n",
       "           1005,  2000,  4785,  2689, 16770,  1024,  1013,  1013,  1056,  1012,\n",
       "           2522,  1013,  1043,  2549,  2140,  2575,  2480, 28311, 13668,  2497,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  7938,  2000,  5770,  2013, 14021, 14526,  2509, 24700, 21358,\n",
       "          18939,  4785,  2689, 14433, 16770,  1024,  1013,  1013,  1056,  1012,\n",
       "           2522,  1013, 10768,  2213, 12031,  2094,  3501, 25708,  2094,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101, 19387,  1030, 12670, 26654, 16700,  2078,  1024, 25614,  1024,\n",
       "           6221,  8398,  2003,  4785,  2689,  1517,  2002,  1521,  1055,  2019,\n",
       "           2006, 18935,  7071,  2008,  1996,  2175,  2361, 10220,  2000,  2227,\n",
       "           1012, 16770,  1024,  1013,  1013,  1056,  1012,  2522,  1013,  1062,\n",
       "           2575,  7875, 20348,  2243,  1529,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32),\n",
       " tensor([1, 2, 0, 1])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VETY-BfUsUhA"
   },
   "source": [
    "# Homework\n",
    "Congratulations on making it to the end of this lesson!\n",
    "\n",
    "As a homework, go ahead and build validation dataloader. A warning though, you cannot use RandomSampler on the validation dataset. Do you know why? What else can we use? Think about it before you check the next notebook out! If you can't figure it out, don't worry, you will know it in the next notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAd4SqnG6Zei"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "L02: Loading Custom Datasets.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
