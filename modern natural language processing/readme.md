# Introduction
This module contains notebooks and datasets for the popular **Modern Natural Language Processing** course at [AISC](https://ai.science/). Original content is developed by:
- Ehsan Amjadian, Director of AI & Technology at RBC
- Royal Sequeira, AI Research Scientist at LG Electronics AI Lab

# Dependencies

To successfully run all the notebooks, you need to have the following packages installed:

- NumPy
- nltk
- pandas
- Matplotlib
- PyTorch
- Transformers
- tensorboard

# Content

## Notebooks

- `L01_transformers_intro.ipynb` demonstrates fundamentals of transformers library
- `L02 Loading Custom Datasets.ipynb` covering topics such as pre-training, finetuning and transfer learning
- `L03_Sentence_Classification.ipynb` fine-tuning of BERT for a sentence-level classification task
- `L04_PyTorch_lightning.ipynb` introduction to PyTorch Lightning 
- `L05_sequence_classification.ipynb` finetuning BERT for token-level classification
- `Notebook_2_NLP_Fundamentals_&_Exercise.ipnyb` explores the fundamentals of NLP using nltk
- `Notebook_3_Low_level_Text_Processing.ipynb` introduces low-level text processing using nltk
- `Notebook_NLP_Transformer_Tutorial.ipynb` training a Transformer model to translate Portuguese into English
- `Notebook_The_Annotated_Transformer.ipynb` introducing **Attention is All You Need** paper
- `Statistical_NLP.ipynb` discussing statistical NLP models

# Additional Reading Materials
- how backpropagation works
	- Academic version: http://neuralnetworksanddeeplearning.com/chap2.html
	- personal favorite (3Blue1Brown): https://youtu.be/Ilg3gGewQ5U
- intro to Autoencoders
	https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368
- the illustrated Transformer
	http://jalammar.github.io/illustrated-transformer/
