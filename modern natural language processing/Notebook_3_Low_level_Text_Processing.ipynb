{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook_3_Low_level_Text_Processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4EnGqn8mZ2l"
      },
      "source": [
        "\n",
        "#Notebook 3. Low-level Text Processing\n",
        "---\n",
        "##Learning Outcomes\n",
        "- Word tokenization\n",
        "- Constructing NLTK text object\n",
        "- Clean text from HTML\n",
        "- Searching a string\n",
        "- Noisy text from the web\n",
        "- Using NLTK to process extracted text\n",
        "- Note on search results\n",
        "- Process rss feeds\n",
        "- File local operations\n",
        "- Simple string operations\n",
        "- Processing PDFs\n",
        "- Character distribution\n",
        "- Finding position & context of a word\n",
        "- Text encodings: unicode, code points, ordinal, hexadecimal, excape sequence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcl4f2AEm2NC"
      },
      "source": [
        "## Libraries Required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uiz-mflfpizi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844119b5-226a-4be8-a152-5c04c9577234"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> book\n",
            "    Downloading collection 'book'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet.zip.\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | \n",
            "     Done downloading collection book\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K76eGUP3r3WS"
      },
      "source": [
        "# If you like to install Conda instead of PIP you can install either the full version of Anaconda or the light version called MiniConda\n",
        "# #installing full Anaconda\n",
        "# !wget https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh && bash Anaconda3-5.2.0-Linux-x86_64.sh -bfp /usr/local\n",
        "# # To make Python find the modules run:\n",
        "# import sys\n",
        "# sys.path.append('/usr/local/lib/python3.6/site-packages')\n",
        "\n",
        "# # Installing Miniconda\n",
        "# !wget https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh && bash Miniconda3-4.5.4-Linux-x86_64.sh -bfp /usr/local\n",
        "# import sys\n",
        "# sys.path.append('/usr/local/lib/python3.6/site-packages')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neB3CJOepmMD"
      },
      "source": [
        "## Text from the Web"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E1b5zHa0qVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429d03f6-3c98-4a77-e155-78b264990c52"
      },
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode(\"utf-8\")\n",
        "print(type(raw))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hfaB6ES1UyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "305540fc-6e57-4f1c-deb8-816763130834"
      },
      "source": [
        "num_of_chars = len(raw)\n",
        "print(\"number of chars in the retrieved text:\", num_of_chars)\n",
        "print(raw[:75])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of chars in the retrieved text: 1176967\n",
            "﻿The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALAkvkN42dNU"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YgWQ2CU3TKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ce91f0-75d2-41b8-c177-0c6f1c740495"
      },
      "source": [
        "tokens = word_tokenize(raw)\n",
        "print(\"Type of tokens is: \"+str(type(tokens)))\n",
        "num_of_tokens = len(tokens)\n",
        "print(\"number of words in the retrieved text is\", num_of_tokens)\n",
        "average_token_length = num_of_chars/num_of_tokens\n",
        "print(\"average token length in the text is\", average_token_length)\n",
        "print(tokens[1:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of tokens is: <class 'list'>\n",
            "number of words in the retrieved text is 257727\n",
            "average token length in the text is 4.566719823689408\n",
            "['Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAfIMQQf391k"
      },
      "source": [
        "###Constructing an NLTK Text Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGlRCV3maeRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b9f1d2-61e0-40c1-b042-893c98a01377"
      },
      "source": [
        "text = nltk.Text(tokens)\n",
        "print(type(text))\n",
        "print(text[1024:1062])\n",
        "print(text.collocations()) # collocations are words that occur frequenctly together i.e., 'Barak' 'Obama' is an example"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'nltk.text.Text'>\n",
            "['an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge', '.', 'He', 'had', 'successfully']\n",
            "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
            "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
            "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
            "great deal; young man; Nikodim Fomitch; Ilya Petrovitch; Project\n",
            "Gutenberg; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
            "heavens\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0UP77IUcgaL"
      },
      "source": [
        "###Search Strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyVgKg0idgfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "191e71ae-a456-4cc7-b059-8841e34997ab"
      },
      "source": [
        "raw.find(\"Dmitri Prokofitch\") # searches from the begining"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "461252"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vPEdpyCeROT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad45e8d9-84c6-44ce-972f-07b699ece433"
      },
      "source": [
        "raw.rfind(\"Dmitri Prokofitch\") # searches from the end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1087973"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygpfDLe7eWf4"
      },
      "source": [
        "###Noisy Text from the Web\n",
        "The text you retrieve from the web isn't always as clean as above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZSAgLNhfK92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "13efe756-8775-4f64-cb27-ba460324a9e2"
      },
      "source": [
        "url = \"https://www.cbc.ca/news/canada/british-columbia/6-new-covid-19-infections-in-b-c-as-virus-spreads-inside-care-home-1.5489921\"\n",
        "html = request.urlopen(url).read().decode('utf8')\n",
        "html[:200]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<!DOCTYPE html>\\n    <html lang=\"en\">\\n        <head>\\n            <title data-react-helmet=\"true\">6 new COVID-19 infections in B.C. as virus spreads inside care home | CBC News</title>\\n            <meta'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6-p8GWzRQOz"
      },
      "source": [
        "### Get Clean Text from HTML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLIJlWx-f91a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e36662-3405-4dd5-d1b6-593dfc8d6b63"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
        "tokens = word_tokenize(raw)\n",
        "tokens[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['6',\n",
              " 'new',\n",
              " 'COVID-19',\n",
              " 'infections',\n",
              " 'in',\n",
              " 'B.C',\n",
              " '.',\n",
              " 'as',\n",
              " 'virus',\n",
              " 'spreads',\n",
              " 'inside',\n",
              " 'care',\n",
              " 'home',\n",
              " '|',\n",
              " 'CBC',\n",
              " 'News',\n",
              " '!',\n",
              " '(',\n",
              " 'function',\n",
              " '(']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf46FScDglyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c1a59f7-95a4-46c3-b93b-07986084dd77"
      },
      "source": [
        "text = nltk.Text(tokens)\n",
        "text.concordance(\"virus\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Displaying 25 of 28 matches:\n",
            "                                     virus spreads inside care home | CBC News \n",
            " '' ImageObject '' , '' name '' : '' Virus Outbreak California '' , '' descript\n",
            " new COVID-19 infections in B.C . as virus spreads inside care home '' , '' pub\n",
            " new COVID-19 infections in B.C . as virus spreads inside care home '' , '' art\n",
            " new COVID-19 infections in B.C . as virus spreads inside care home '' , '' sha\n",
            " new COVID-19 infections in B.C . as virus spreads inside care home '' , '' ori\n",
            "enry said & nbsp ; on Saturday . The virus spread after a worker contracted the\n",
            "spread after a worker contracted the virus earlier this week.\\u003C\\u002Fp\\u003\n",
            " workers at the centre to ensure the virus has n't spread to other parts of the\n",
            "nd steps it 's taking to prevent the virus from spreading.\\u003C\\u002Fp\\u003E \\\n",
            " woman in their 60s — contracted the virus while \\u003Ca href=\\ '' https : \\u00\n",
            "n B.C . have tested positive for the virus . At least four have recovered. & nb\n",
            "ame infected . `` , '' title '' : '' Virus Outbreak California '' , '' type '' \n",
            "o Chronicle '' , '' headline '' : '' Virus Outbreak California '' , '' size '' \n",
            " new COVID-19 infections in B.C . as virus spreads inside care home | CBC News \n",
            " '' ImageObject '' , '' name '' : '' Virus Outbreak California '' , '' descript\n",
            " new COVID-19 infections in B.C . as virus spreads inside care home '' , '' pub\n",
            " new COVID-19 infections in B.C . as virus spreads inside care home '' , '' art\n",
            " new COVID-19 infections in B.C . as virus spreads inside care home | CBC News \n",
            " new COVID-19 infections in B.C . as virus spreads inside care home | CBC News \n",
            " new COVID-19 infections in B.C . as virus spreads inside care homeSix more peo\n",
            " Bonnie Henry said on Saturday . The virus spread after a worker contracted the\n",
            "spread after a worker contracted the virus earlier this week . Another pair wer\n",
            " workers at the centre to ensure the virus has n't spread to other parts of the\n",
            "nd steps it 's taking to prevent the virus from spreading . She said she 's not\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNClqbYomZio"
      },
      "source": [
        "###A note on Search Engines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-wLgrYGmiYl"
      },
      "source": [
        "NOTE: Unlike text objects, search results are not static and change every now and then. They are also differ regionally. No single pattern is expected to work across the board."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi_BOmS3qVsH"
      },
      "source": [
        "###RSS Feeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_mMPsSLrcB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9557671-4817-4176-82c5-7f93ed19cc1d"
      },
      "source": [
        "!pip install feedparser # package to parse RSS Feeds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (6.0.2)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Zhl8LgsLia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082877fb-a9b7-440a-9724-bc91b8aef37f"
      },
      "source": [
        "import feedparser\n",
        "NewsFeed = feedparser.parse(\"https://www.cbc.ca/cmlink/rss-topstories\")\n",
        "print('Number of RSS posts :', len(NewsFeed.entries))\n",
        "entry = NewsFeed.entries[1]\n",
        "\n",
        "entry.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of RSS posts : 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['title', 'title_detail', 'links', 'link', 'id', 'guidislink', 'published', 'published_parsed', 'authors', 'author', 'tags', 'summary', 'summary_detail'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myPNZSQrtQU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabfd355-eb45-4043-932d-e4751d2cbcaa"
      },
      "source": [
        "print('Post Title :', entry.title)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Post Title : TRC commissioners call on Ottawa to end delays in implementing Calls to Action\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX--zs0NvBUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5901e67d-c147-4989-82ae-11e366fa3433"
      },
      "source": [
        "print(entry.published) # publication time\n",
        "print(\"******\")\n",
        "print(\"------News Link--------\")\n",
        "print(entry.link)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri, 28 May 2021 20:07:30 EDT\n",
            "******\n",
            "------News Link--------\n",
            "https://www.cbc.ca/news/politics/trc-commissioners-calls-to-action-redouble-efforts-1.6051580?cmp=rss\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpboazT7vzfE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e788775d-f2b4-4cfc-90e4-b176976ddeb2"
      },
      "source": [
        "#summary = request.urlopen(entry.link).read().decode('utf8')\n",
        "summary = entry.summary\n",
        "raw = BeautifulSoup(summary, 'html.parser').get_text()\n",
        "raw[:50]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4HOJkghwdE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd08c7f-1d25-4670-babc-b58840088a44"
      },
      "source": [
        "tokens = word_tokenize(raw)\n",
        "tokens[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y24AbdQiw8J1"
      },
      "source": [
        "###Local File Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TefF-7aFxlYJ"
      },
      "source": [
        "f = open(\"document.txt\") # it will give an error, as the file doesn't exist at this point\n",
        "raw = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvUSUESD8xkA"
      },
      "source": [
        "import os\n",
        "os.listdir(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INsA_GxH_HN1"
      },
      "source": [
        "f = open(\"document.txt\", \"w\")\n",
        "f.write('Time flies like an arrow.\\nFruit flies like a banana.\\n')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQR8c0XK_2Yu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05427ec1-d25b-4ee0-e9ef-655d9e8fceda"
      },
      "source": [
        "with open(\"document.txt\", \"r\") as f:\n",
        "  for line in f:\n",
        "    print(line.strip())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time flies like an arrow.\n",
            "Fruit flies like a banana.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyAYFfkxAsde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59f9332-ddfa-4ac5-a4a9-95860153de27"
      },
      "source": [
        "path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')\n",
        "raw = open(path, 'r').read()\n",
        "len(raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1220066"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJb4CyYFBJ9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e8aaad1-c20b-469d-a393-2c14782192f0"
      },
      "source": [
        "type(raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dENVT1j-Dbwy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b30f3df-1ad8-402e-efd9-d0b083d39afd"
      },
      "source": [
        "raw[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[Moby Dick by Herman Melville 1851]\\n\\n\\nETYMOLOGY.\\n\\n(Supplied by a Late Consumptive Usher to a Grammar'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tf4EnCjDfWB"
      },
      "source": [
        "###Processing PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMmi_OpgD6MA"
      },
      "source": [
        "ASCII text and HTML --> Human Readable\n",
        "\n",
        "PDF --> Binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m8Tv1fTEL0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae310d9-0970-47b4-adc3-228cf35e40f8"
      },
      "source": [
        "!pip install msgpack # dependency for slate3k & pypdf2\n",
        "!pip install slate3k # package to extract text from PDF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Collecting slate3k\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/e3/f27cac1dd24617894cf7ddb5da13beca27c9236736466bebaf5dd2a902c1/slate3k-0.5.3-py2.py3-none-any.whl\n",
            "Collecting pdfminer3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/15/5ac4faa314c38b335cf4db37fc02dc02c14bf67f7641bea2fa5e5b7d4ff4/pdfminer3k-1.3.4-py3-none-any.whl (100kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.4MB/s \n",
            "\u001b[?25hCollecting ply\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/58/35da89ee790598a0700ea49b2a66594140f44dec458c07e8e3d4979137fc/ply-3.11-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hInstalling collected packages: ply, pdfminer3k, slate3k\n",
            "Successfully installed pdfminer3k-1.3.4 ply-3.11 slate3k-0.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaX7SYquXSi1"
      },
      "source": [
        "####Access a PDF from the Web & Convert it to Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywcNnaB3XeVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b30630-9d9c-4922-cba8-bde3581b5e75"
      },
      "source": [
        "import urllib\n",
        "import slate3k as slate\n",
        "from io import StringIO, BytesIO\n",
        "\n",
        "url = \"https://curve.carleton.ca/system/files/etd/4476fc9c-bfa6-49a0-bf73-8045bf299af8/etd_pdf/4ca4228e73e35eef6d1ee4ad8f308152/amjadian-representationlearningforinformationextraction.pdf\"\n",
        "\n",
        "response = request.urlopen(url)\n",
        "rawPDF = response.read()\n",
        "memoryFile = BytesIO(rawPDF) # to create a file in memory to read from\n",
        "\n",
        "# extract text with slate\n",
        "document1 = slate.PDF(memoryFile)\n",
        "print(\"SLATE:\\n\", document1[1], \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:pdfminer.layout:Too many boxes (471) to group, skipping.\n",
            "WARNING:pdfminer.layout:Too many boxes (703) to group, skipping.\n",
            "WARNING:pdfminer.layout:Too many boxes (684) to group, skipping.\n",
            "WARNING:pdfminer.layout:Too many boxes (393) to group, skipping.\n",
            "WARNING:pdfminer.layout:Too many boxes (231) to group, skipping.\n",
            "WARNING:pdfminer.layout:Too many boxes (163) to group, skipping.\n",
            "WARNING:pdfminer.layout:Too many boxes (380) to group, skipping.\n",
            "WARNING:pdfminer.layout:Too many boxes (104) to group, skipping.\n",
            "WARNING:pdfminer.layout:Too many boxes (103) to group, skipping.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SLATE:\n",
            " Abstract\n",
            "\n",
            "Distributed representations, predominantly acquired via neural networks, have\n",
            "\n",
            "been applied to natural\n",
            "\n",
            "language processing tasks including speech recognition and\n",
            "\n",
            "machine translation with a success comparable to sophisticated state-of-the-art algo-\n",
            "\n",
            "rithms. The present thesis oﬀers an investigation of the application of such represen-\n",
            "\n",
            "tations\n",
            "\n",
            "to information extraction.\n",
            "\n",
            "Speciﬁcally,\n",
            "\n",
            "I explore the suitability of applying\n",
            "\n",
            "shallow distributed representations to the automatic terminology extraction task, as\n",
            "\n",
            "well as the bridging reference resolution task.\n",
            "\n",
            "I created a dataset as a gold standard\n",
            "\n",
            "for automatic term extraction in the mathematical education domain.\n",
            "\n",
            "I carefully as-\n",
            "\n",
            "sessed the performance of the existing terminology extraction methods on this dataset.\n",
            "\n",
            "Then, I introduce a novel method for automatic terminology extraction for one word\n",
            "\n",
            "terms, and I evaluate the performance of the novel algorithm in various terminological\n",
            "\n",
            "domains. The introduced algorithm leverages the distributed representation of words\n",
            "\n",
            "from the local and global perspectives to encode syntactic, semantic, association, and\n",
            "\n",
            "frequency information at\n",
            "\n",
            "the same time. Furthermore,\n",
            "\n",
            "this novel algorithm can be\n",
            "\n",
            "trained with a minimal number of data points.\n",
            "\n",
            "I show that the algorithm is robust\n",
            "\n",
            "to the change of domain, and that information can be transferred from one technical\n",
            "\n",
            "domain to another,\n",
            "\n",
            "leveraging what we call anchor words with consistent semantics\n",
            "\n",
            "shared between the domains. As for the bridging reference resolution task, a dataset\n",
            "\n",
            "is built on the letter portion of the Open American National Corpus and I compare\n",
            "\n",
            "the performance of a preliminary method against a ma jority class baseline.\n",
            "\n",
            "ii\n",
            "\n",
            "\f \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jumBHNcDu7bP"
      },
      "source": [
        "#####For additional ways to process a PDF file see: http://www.blog.pythonlibrary.org/2018/05/03/exporting-data-from-pdfs-with-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqLLT4YoxLTQ"
      },
      "source": [
        "###Character Distribution\n",
        "Character distribution is a significatn feature for language detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np6u5yXqWmLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a61152f-ea41-4ad3-f004-cf64b3fee2ec"
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "doc1_text = document1.text()\n",
        "fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha()) # frequency distribution of the alphabetic chars\n",
        "most_common = fdist.most_common(5) # 5 most frequent chars\n",
        "print(most_common)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('e', 117092), ('t', 87996), ('a', 77916), ('o', 69326), ('n', 65617)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OmU4BPAW7rX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4363f4-7152-420b-d512-1e1557762473"
      },
      "source": [
        "doc1_text.find(\"NLP\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3740"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK6IPka0Y2Do",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66f70187-7d20-40ac-ee24-273fab7e8276"
      },
      "source": [
        "doc1_text[3700:3800]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'piring discussions and conversations in NLP, iv information extraction, word embeddings, and high di'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhF6NkpiY7Jg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a98b64c6-248e-4f86-b208-db7173491d23"
      },
      "source": [
        "tokenized_text = word_tokenize(doc1_text)\n",
        "text = nltk.Text(tokenized_text)\n",
        "text.concordance(\"NLP\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Displaying 12 of 12 matches:\n",
            "ring discussions and conversations in NLP , iv information extraction , word em\n",
            "tion to all the other tasks in modern NLP . A more concrete consequence of this\n",
            " to improve and simplify a variety of NLP tasks ( Collobert and Weston , 2008 ;\n",
            "ﬁeld of Natural Language Processing ( NLP ) , Automatic Terminology Extrac- tio\n",
            "ction has many direct applications in NLP , such as information 3.1 . INTRODUCT\n",
            " , the rise of distributed methods in NLP , especially the recent word embeddin\n",
            "owards domain-independent distributed NLP . A new dataset representing four new\n",
            "ed Bridging Reference Resolution Many NLP systems treat deﬁnite noun phrases as\n",
            "elf . It has been shown previously in NLP that words can be mapped across langu\n",
            "egral part of a variety of downstream NLP tasks . We plan to employ our methods\n",
            "olution : To What Extent Does It Help NLP Applications ? , pages 16–27 . Spring\n",
            " 2010 Workshop on New Chal lenges for NLP Frameworks , pages 45–50 , Valletta ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr6J-WURZbY4"
      },
      "source": [
        "##Unicode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY81xMVzgA6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54a54eb2-7e9f-445e-aeaa-9929ace599a6"
      },
      "source": [
        "# Unicode View\n",
        "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
        "f = open(path, encoding='latin2')\n",
        "for line in f:\n",
        "  line = line.strip()\n",
        "  print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\n",
            "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\n",
            "Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\n",
            "odnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\n",
            "Jagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\n",
            "archiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyUGL8FJ9y2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574e0995-f8b3-47b5-b8a1-0d370a5945e0"
      },
      "source": [
        "# Viewing the Code Points\n",
        "f = open(path, encoding='latin2')\n",
        "for line in f:\n",
        "  line = line.strip()\n",
        "  print(line.encode('unicode_escape'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Pruska Biblioteka Pa\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\u0105'\n",
            "b'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez'\n",
            "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y'\n",
            "b'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\u0142y do Biblioteki'\n",
            "b'Jagiello\\\\u0144skiej w Krakowie, obejmuj\\\\u0105 ponad 500 tys. zabytkowych'\n",
            "b'archiwali\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2IFc6p7-MoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "964b95e1-2f18-4ca5-b634-e013dba7926b"
      },
      "source": [
        "#Ordinal of a char\n",
        "ord('ń')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "324"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjS9EdcgC4vE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b297c6ba-7be8-4e85-b6de-c9cf78bdf071"
      },
      "source": [
        "hex(324)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0x144'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC6IdVde_cgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "196c8f5a-1f1c-4e4a-b273-4f6fc29364ff"
      },
      "source": [
        "nacute = '\\u0144' # escape sequence to define a string\n",
        "print(type(nacute), nacute)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'> ń\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhlS1K3O_c_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d72b8e-195c-4835-93cb-dfbb7f81e13f"
      },
      "source": [
        "type(nacute.encode(\"utf-8\") )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bytes"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Ax5Ks2_mrf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c172e4c9-4fb2-486f-e0c6-e260fe43716a"
      },
      "source": [
        "nacute"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ń'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02lN1dZP_rgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd51f84-6c9e-4ad7-8a1f-b7b8a7d5d781"
      },
      "source": [
        "# UTF-8 byte sequence, followed by their code point integer using the standard Unicode convention (i.e., prefixing the hex digits with U+), followed by their Unicode name\n",
        "import unicodedata\n",
        "c = 'ń'\n",
        "print(\"Char: \", c, \"\\nUTF-8 byte seq:\", c.encode(\"utf8\"), \"\\nCode point integer:\", ord(c), \"\\nUnicode name:\", unicodedata.name(c))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Char:  ń \n",
            "UTF-8 byte seq: b'\\xc5\\x84' \n",
            "Code point integer: 324 \n",
            "Unicode name: LATIN SMALL LETTER N WITH ACUTE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF6ahib9QHBd"
      },
      "source": [
        "**Note: NLTK tokenizer allows Unicode strings as input and as output**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuzvgBtYOq-h"
      },
      "source": [
        "# Exercise 3\n",
        "## a. What does document1[1][-1] retrieve?\n",
        "## b. Exercise: what does document1[1][-20:-4] retrieve?\n",
        "## c. What is the type of document1?\n",
        "## d. Tokenize document1.\n",
        "## f. Find 2 web pages, each belonging to a different language. Use their char distribution to detect the language they belong to. Hint: you can build a table representing the relative frequency of each char to help with the detection.\n",
        "## g. Find 2 web pages, each belonging to a different language. Use their encodings to detect their lanugage. \n",
        "## h. Would it help if you use a combination of the methods suggested in \"f\" and \"g\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teq2KDfz1zjW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}